{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Torchtext Beginner Project"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this project you need to install torchtext and spaCy. So, if you already have it then you can carry on, but if you dont please use the following command in `conda powershell prompt` to install it.\n",
    "\n",
    " \n",
    "- [ ]  `pip install torchtext`\n",
    "- [ ]  `pip install spacy`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With that concluded, what we need to is to get the Twitter Dataset: [Sentiment140](https://www.kaggle.com/kazanova/sentiment140)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, lets begin our project.\n",
    "\n",
    "We will start by importing the packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Importing necessary packages ##\n",
    "\n",
    "import spacy\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torchtext\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now with that lets load our dataset.\n",
    "\n",
    "Since it is a csv file we are going to use `pandas` to import it using the following code.\n",
    "\n",
    "```python\n",
    "pandas.read_csv('training.1600000.processed.noemoticon.csv')\n",
    "```\n",
    "\n",
    "But that would raise an error:\n",
    "\n",
    "```python\n",
    "'utf-8' codec can't decode bytes in position 7970-7971: invalid continuation byte after engine=python\n",
    "```\n",
    "\n",
    "So for that we need to change the engine to `python` and encoding to `ISO-8859-1`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Importing the dataset ##\n",
    "\n",
    "tweets = pd.read_csv('training.1600000.processed.noemoticon.csv' ,\n",
    "                     engine = 'python',\n",
    "                     encoding = 'ISO-8859-1',\n",
    "                     names = ['score' , 'id' , 'date' , 'query' , 'name' , 'tweet'],\n",
    "                     header = None)\n",
    "\n",
    "## Displaying first 5 rows ##\n",
    "tweets.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now with that out of the way, lets decode the dataset a bit.\n",
    "\n",
    "The column 0 is basically the target sentiment and the column 5 are the tweets.\n",
    "\n",
    "Lets check the unique values of sentiments and how many values there are altogether."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Checking number of class values ##\n",
    "\n",
    "tweets['score'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wow! The dataset is a gem.\n",
    "\n",
    "It has evenly distributed classes each with values 80000.\n",
    "\n",
    "The classes are 0 which equivalents to negative sentiment and 4 which is poitive sentiment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now since the values are 0 and 4 we can transform them into categorical datatypes and thereafter move ahead."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "## Transforming column 0 ##\n",
    "\n",
    "tweets['sentiment_cat'] = tweets['score'].astype('category')\n",
    "\n",
    "tweets['sentiment'] = tweets['sentiment_cat'].cat.codes.astype('float')\n",
    "\n",
    "tweets.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets check the values now!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets['sentiment'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets save this as a new csv file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Saving the dataset ##\n",
    "\n",
    "tweets.to_csv('twitter_data.csv' , index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now to move ahead we need to make fields.\n",
    "\n",
    "What does fields do?\n",
    "\n",
    "Well they take sequence data when given and tokenize them. \n",
    "\n",
    "Furthermore there are LabelFields which contibute to the label recognition.\n",
    "\n",
    "SO, lets do those things."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Creating Field objects for the data ##\n",
    "\n",
    "spacy_en = spacy.load('en_core_web_sm')\n",
    "\n",
    "def token(text):\n",
    "    return [tok.text for tok in spacy_en.tokenizer(text)]\n",
    "\n",
    "label = torchtext.data.LabelField()\n",
    "\n",
    "tweet = torchtext.data.Field(tokenize = token , \n",
    "                             lower = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now lets move ahead and formulate our torchtext dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Setting our dataset ##\n",
    "\n",
    "fields = [('score' , None) , ('id' , None) , ('date' , None) , ('query' , None) ,\n",
    "          ('name' , None) , ('tweet' , tweet) , ('sentiment_cat' , None) , ('sentiment' , label)]\n",
    "\n",
    "tweet_dataset = torchtext.data.TabularDataset(path = 'twitter_data.csv',\n",
    "                                              format = 'CSV' ,\n",
    "                                              fields = fields,\n",
    "                                              skip_header = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Boom!! We have our dataset.\n",
    "\n",
    "Lets split the dataset into three parts: training , testing , validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Splitting the dataset ##\n",
    "\n",
    "(train , val , test) = tweet_dataset.split(split_ratio = [0.8 , 0.1 , 0.1])\n",
    "\n",
    "## Building Vocabulary ##\n",
    "\n",
    "tweet.build_vocab(train , max_size = 50000)\n",
    "label.build_vocab(train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now lets build the iterator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Setting device##\n",
    "\n",
    "device = torch.device('cuda')\n",
    "\n",
    "## Building iterator ##\n",
    "\n",
    "train_iterator , val_iterator , test_iterator = torchtext.data.BucketIterator.splits(datasets = (train , val , test),\n",
    "                                                                                     batch_size = 32 ,\n",
    "                                                                                     device = device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, its time that we build our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Building our model ##\n",
    "\n",
    "class SentimentModel(nn.Module):\n",
    "    \n",
    "    def __init__(self , hidden_size , embedding_dim , vocab_size):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(50000 , embedding_dim)\n",
    "        self.encoder = nn.LSTM(input_size = embedding_dim ,\n",
    "                               hidden_size = hidden_size ,\n",
    "                               num_layers = 1)\n",
    "        self.predictor = nn.Linear(hidden_size , 1)\n",
    "        \n",
    "    def forward(self , x):\n",
    "        out = self.embedding(x)\n",
    "        out , (hidden , _) = self.encoder(out)\n",
    "        pred = self.predictor(hidden.squeeze(0))\n",
    "        return pred\n",
    "    \n",
    "\n",
    "tweet_model = SentimentModel(100 , 300 , 20000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Done!! Our model is set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets set the optimizer and loss function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Optimizer ##\n",
    "\n",
    "optim = torch.optim.Adam(tweet_model.parameters() , lr = 3e-4)\n",
    "\n",
    "## Loss Function ##\n",
    "criterion = nn.BCEWithLogitsLoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets train our model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But first moving the model to cuda."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Moving device to cuda ##\n",
    "\n",
    "device = torch.device('cuda')\n",
    "\n",
    "tweet_model = tweet_model.to(device = device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Training ##\n",
    "\n",
    "num_epochs = 10\n",
    "\n",
    "train_loss = []\n",
    "val_loss = []\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    \n",
    "    tweet_model.train()\n",
    "    for batch in train_iterator:\n",
    "        pred = tweet_model(batch.tweet)\n",
    "        label = batch.sentiment.reshape(-1 , 1).type(torch.cuda.FloatTensor)\n",
    "        #print(label.type())\n",
    "        #print(pred.type())\n",
    "        optim.zero_grad()\n",
    "        loss = criterion(pred, label)\n",
    "        loss.backward()\n",
    "        optim.step()\n",
    "    \n",
    "    train_loss.append(loss.item())\n",
    "    \n",
    "    tweet_model.eval()\n",
    "    for batch in val_iterator:\n",
    "        pred = tweet_model(batch.tweet)\n",
    "        label = batch.sentiment.reshape(-1 , 1).type(torch.cuda.FloatTensor)\n",
    "        loss = criterion(pred , label)\n",
    "    \n",
    "    val_loss.append(loss.item())\n",
    "    \n",
    "    print('Epoch : {} / {} --> Training Loss : {:.3f} , Validation Loss : {:.3f}'.format(epochs + 1 , num_epochs , train_loss[epoch] , val_loss[epoch]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Awesome!!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Thank You!!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch_env",
   "language": "python",
   "name": "pytorch_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
