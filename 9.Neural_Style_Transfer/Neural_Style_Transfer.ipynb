{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural Style Transfer Implementation using Pytorch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook we are going to implement Neural Style Transfer and we are going to showcase it on my city Kolkata's one of the famous places Victoria Memorial Hall and we are going to paint it like Van Gogh."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now in this notebook we are going to utilize the VGG19 as our base model.\n",
    "\n",
    "So, lets get started with all the imports."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Importing necessary packages ##\n",
    "\n",
    "import torch\n",
    "import torch.nn\n",
    "import torchvision\n",
    "from torchvision.models import vgg19\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "import PIL\n",
    "import numpy as np\n",
    "import matplotlib as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With all the packages imported, lets check out the vgg19 model.\n",
    "\n",
    "We basically dont need the entire model. We just need some parts of it. Just few convolutional layer outputs.\n",
    "\n",
    "So we will check the model and then set our requirements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sequential(\n",
       "  (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (1): ReLU(inplace=True)\n",
       "  (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (3): ReLU(inplace=True)\n",
       "  (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  (5): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (6): ReLU(inplace=True)\n",
       "  (7): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (8): ReLU(inplace=True)\n",
       "  (9): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  (10): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (11): ReLU(inplace=True)\n",
       "  (12): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (13): ReLU(inplace=True)\n",
       "  (14): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (15): ReLU(inplace=True)\n",
       "  (16): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (17): ReLU(inplace=True)\n",
       "  (18): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  (19): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (20): ReLU(inplace=True)\n",
       "  (21): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (22): ReLU(inplace=True)\n",
       "  (23): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (24): ReLU(inplace=True)\n",
       "  (25): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (26): ReLU(inplace=True)\n",
       "  (27): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  (28): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (29): ReLU(inplace=True)\n",
       "  (30): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (31): ReLU(inplace=True)\n",
       "  (32): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (33): ReLU(inplace=True)\n",
       "  (34): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (35): ReLU(inplace=True)\n",
       "  (36): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       ")"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Checking vgg19 model ##\n",
    "\n",
    "model_layers = vgg19(pretrained = True).features\n",
    "\n",
    "model_layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can see the model.\n",
    "\n",
    "The layers that we need are 0,5,10,19 and 28.\n",
    "\n",
    "So, we are going to set it like that."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, how are we going to do that?\n",
    "\n",
    "Well we are going to get the output with regards to those layers.\n",
    "\n",
    "Lets set our model and do that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Setting our custom model ##\n",
    "\n",
    "class NeuralStyleNet(torch.nn.Module):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.model = vgg19(pretrained = True).features[:29]\n",
    "        self.feature_layers = [0,5,10,19,28]\n",
    "        \n",
    "    def forward(self , x):\n",
    "        outputs = []\n",
    "        \n",
    "        for layer_num , layer_name in enumerate(self.model):\n",
    "            \n",
    "            x = layer_name(x)\n",
    "            \n",
    "            if layer_num in self.feature_layers:\n",
    "                outputs.append(x)\n",
    "        \n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And with that we have sorted out our model.\n",
    "\n",
    "Next up lets instantiate it and send it to gpu."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Setting device ##\n",
    "\n",
    "def set_device():\n",
    "    if torch.cuda.is_available():\n",
    "        return torch.device('cuda')\n",
    "    return torch.device('cpu')\n",
    "\n",
    "device = set_device()\n",
    "\n",
    "\n",
    "## Instantiating the model ##\n",
    "\n",
    "neural_style_net = NeuralStyleNet().to(device).eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And the model is instantiated.\n",
    "\n",
    "Now what we need to do is to import our content and style images and transfer them as tensors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Image loading utility function ##\n",
    "\n",
    "def image_loader(img_path):\n",
    "    \n",
    "    img = PIL.Image.open(img_path)\n",
    "    \n",
    "    trans = transforms.Compose([\n",
    "        transforms.Resize((256 , 256)),\n",
    "        transforms.ToTensor()\n",
    "    ])\n",
    "    \n",
    "    img_tensor = trans(img).unsqueeze(0).to(device)\n",
    "    \n",
    "    return img_tensor\n",
    "\n",
    "## Getting the style and content images ##\n",
    "\n",
    "content = image_loader('victoria.jpg')\n",
    "style = image_loader('van_gogh.jpg')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And done!!\n",
    "\n",
    "Now we need to set our generated image. \n",
    "\n",
    "Normally in the original paper they set it to random noise. But for ease of training we are going to get a copy of our content image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Generated image ##\n",
    "\n",
    "gen_img = content.clone().requires_grad_(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now what we need to do is set our loss and optimizer as well as the alpha and beta hyperparameters which are necessary for our final loss function.\n",
    "\n",
    "Now for defining the loss we would define some utility function, so lets do that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Mean Square loss ##\n",
    "\n",
    "def mse(tensor1 , tensor2):\n",
    "    \n",
    "    diff = tensor1 - tensor2\n",
    "    \n",
    "    mean_sq_loss = torch.mean((tensor1 - tensor2) ** 2)\n",
    "    \n",
    "    return mean_sq_loss\n",
    "\n",
    "## Gram Matrix ##\n",
    "\n",
    "def gram_matrix(tensor):\n",
    "    \n",
    "    tensor_transpose = tensor.t()\n",
    "    \n",
    "    g_m = torch.mm(tensor , tensor_transpose)\n",
    "    \n",
    "    return g_m"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now lets define our optimizer and some basic hyperparameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Optimizer ##\n",
    "\n",
    "optim = torch.optim.Adam([gen_img] , lr = 0.001)\n",
    "\n",
    "## Alpha Beta values ##\n",
    "\n",
    "alpha = 1\n",
    "beta = 0.01\n",
    "\n",
    "## Repeat steps ##\n",
    "\n",
    "repeat_steps = 4000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now lets get our hands dirty and train."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 500 / 4000 --> The total loss of the generated image is : 4734.976\n",
      "Step 1000 / 4000 --> The total loss of the generated image is : 2461.896\n",
      "Step 1500 / 4000 --> The total loss of the generated image is : 1793.940\n",
      "Step 2000 / 4000 --> The total loss of the generated image is : 1426.871\n",
      "Step 2500 / 4000 --> The total loss of the generated image is : 1192.800\n",
      "Step 3000 / 4000 --> The total loss of the generated image is : 1030.000\n",
      "Step 3500 / 4000 --> The total loss of the generated image is : 908.405\n",
      "Step 4000 / 4000 --> The total loss of the generated image is : 809.167\n"
     ]
    }
   ],
   "source": [
    "## Training ##\n",
    "\n",
    "for i in range(repeat_steps):\n",
    "    \n",
    "    gen_features = neural_style_net(gen_img)\n",
    "    \n",
    "    style_features = neural_style_net(style)\n",
    "    \n",
    "    content_features = neural_style_net(content)\n",
    "    \n",
    "    content_loss = style_loss = 0\n",
    "    \n",
    "    for gen_feature , style_feature , content_feature in zip(gen_features , style_features , content_features):\n",
    "        \n",
    "        batch , channel , height , width = gen_feature.shape\n",
    "        \n",
    "        content_loss += mse(gen_feature , content_feature)\n",
    "        \n",
    "        gram_gen = gram_matrix(gen_feature.view(channel , height * width))\n",
    "        \n",
    "        gram_style = gram_matrix(style_feature.view(channel , height * width))\n",
    "        \n",
    "        style_loss += mse(gram_gen , gram_style)\n",
    "        \n",
    "    total_loss = alpha * content_loss + beta * style_loss\n",
    "    \n",
    "    optim.zero_grad()\n",
    "    \n",
    "    total_loss.backward()\n",
    "    \n",
    "    optim.step()\n",
    "    \n",
    "    if (i + 1) % 500 == 0:\n",
    "        \n",
    "        print('Step {} / {} --> The total loss of the generated image is : {:.3f}'.format(i + 1 , repeat_steps , total_loss))\n",
    "        \n",
    "        torchvision.utils.save_image(gen_img , 'generated_images/generated_{}.png'.format(i + 1))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch_env",
   "language": "python",
   "name": "pytorch_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
