{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Solving the CIFAR Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So moving on from the basic linear regression problem, in this notebook we are going to improvise on CIFAR-10 Dataset classification problem by utilizing Pytorch."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we need to understand Pytorch is a tensor framework and can't handle images in their original form and has a separate library called torchvision to handle images."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets import the packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from torch import nn\n",
    "from torch.utils.data.dataloader import DataLoader\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "\n",
    "from torchvision.datasets import CIFAR10\n",
    "import torchvision.transforms as transforms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, one cool thing about torchvision is that it itself has a horde of datasets, with Cifar-10 being one of them. We are going to load the dataset from there and also transform the images into tensors so that it can be handled by Pytorch."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One more thing the CIFAR Dataset is RGB in color. For now we are going to utilize only Grayscale images hence we are going to transform them to grayscale."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_transform = transforms.Compose([transforms.Grayscale(num_output_channels = 1),\n",
    "                                     transforms.ToTensor()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "cifa_data = CIFAR10(root = '/data',\n",
    "                    transform = image_transform,\n",
    "                    train = True,\n",
    "                    download = True\n",
    "                   )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Okay! So, the dataset is here.\n",
    "\n",
    "Lets check its length and see one of its elements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50000"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(cifa_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 32, 32])\n",
      "6\n"
     ]
    }
   ],
   "source": [
    "x, y = cifa_data[0]\n",
    "\n",
    "print(x.shape)\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Okay, so the dataset is of 50,000 images of shape 32 * 32. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Its always awesome to visualize one of the pictures before moving ahead. \n",
    "\n",
    "For that we are going to utilize the <code>plt.imshow()</code> function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Label is : 9\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "img , label = cifa_data[1]\n",
    "img = img.reshape(32 , 32)\n",
    "plt.imshow(img , cmap  ='gray')\n",
    "print('The Label is :', label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Okay, it seems like a truck.\n",
    "\n",
    "It is quite evident the images are quite small, but its easier for storage and manipulation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now lets create our model.\n",
    "\n",
    "It will be very simple model with linear layer outputting the output of 10units.\n",
    "\n",
    "Lets do that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class cifar_model(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.linear = nn.Linear(1024 , 10)\n",
    "        \n",
    "    def forward(self , x):\n",
    "        x = x.reshape(-1 , 1024)\n",
    "        out = self.linear(x)\n",
    "        return out\n",
    "    \n",
    "cif_model = cifar_model() #instantiating cifar model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Parameter containing:\n",
       " tensor([[-0.0057, -0.0047, -0.0063,  ..., -0.0207,  0.0124, -0.0301],\n",
       "         [ 0.0040, -0.0235, -0.0031,  ...,  0.0213, -0.0303, -0.0191],\n",
       "         [ 0.0068,  0.0217, -0.0194,  ..., -0.0085, -0.0061,  0.0147],\n",
       "         ...,\n",
       "         [-0.0019,  0.0029, -0.0167,  ..., -0.0115,  0.0095,  0.0308],\n",
       "         [ 0.0273,  0.0267, -0.0241,  ..., -0.0243,  0.0146,  0.0098],\n",
       "         [-0.0133, -0.0176, -0.0057,  ..., -0.0310, -0.0110, -0.0208]],\n",
       "        requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([-0.0026, -0.0042, -0.0016, -0.0147, -0.0009, -0.0236, -0.0118, -0.0266,\n",
       "         -0.0279, -0.0114], requires_grad=True)]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(cif_model.parameters())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now with the model set, it is essential to split the dataset into training and validation. For validation we are going to keep 10% of the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets create a function to pick out the indexes of validation and training sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_val_split(total_len , val_percent):\n",
    "    val_range = int(total_len * val_percent)\n",
    "    shuffled_idx =  np.random.permutation(total_len)\n",
    "    return shuffled_idx[: val_range] , shuffled_idx[val_range: ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "total_len = len(cifa_data)\n",
    "val_percent = 0.10\n",
    "\n",
    "val_idx , train_idx = train_val_split(total_len , val_percent)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets check the validation and training length."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5000\n",
      "45000\n"
     ]
    }
   ],
   "source": [
    "print(len(val_idx))\n",
    "print(len(train_idx))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Yes! Everything is sorted. Now what we do is we set our dataloader with more random samplings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_sampler = SubsetRandomSampler(train_idx)\n",
    "train_dl = DataLoader(cifa_data , batch_size = 512 , sampler = train_sampler)\n",
    "\n",
    "val_sampler = SubsetRandomSampler(val_idx)\n",
    "val_dl = DataLoader(cifa_data , batch_size = 512 , sampler = val_sampler)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now its time to set our loss function and optimizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_func = torch.nn.functional.cross_entropy  ##setting the loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "optim = torch.optim.SGD(cif_model.parameters() , lr = 1e-3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we need to set a metric to validate our model. So we are going to define accuracy function here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(predicted_val , target):\n",
    "    _,pred = torch.max(predicted_val , axis = 1)\n",
    "    return torch.sum(pred == target).item() / len(target)   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now its time to train our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit(model , epochs, dl , loss_func , optim , metric = None):\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        \n",
    "        for x , y in dl:\n",
    "            \n",
    "            ##generate predictions\n",
    "            pred = model(x)\n",
    "            \n",
    "            ##calculate loss function\n",
    "            loss = loss_func(pred , y)\n",
    "            \n",
    "            ##one step of backpropagation\n",
    "            loss.backward()\n",
    "            \n",
    "            ##one step of optimization\n",
    "            optim.step()\n",
    "            \n",
    "            ##reset the gradients to zero\n",
    "            optim.zero_grad()\n",
    "            \n",
    "        if metric is not None:\n",
    "            acc = accuracy(pred , y)\n",
    "            print('Epochs : {} , Loss : {:.4f} , Accuracy : {:.4f}'.format(epoch + 1 , loss , acc))\n",
    "        else:\n",
    "            print('Epochs : {} , Loss : {:.4f}'.format(epoch + 1 , loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs : 1 , Loss : 2.2170 , Accuracy : 0.2259\n",
      "Epochs : 2 , Loss : 2.2097 , Accuracy : 0.2127\n",
      "Epochs : 3 , Loss : 2.2132 , Accuracy : 0.1886\n",
      "Epochs : 4 , Loss : 2.1890 , Accuracy : 0.1952\n",
      "Epochs : 5 , Loss : 2.1917 , Accuracy : 0.2083\n",
      "Epochs : 6 , Loss : 2.1831 , Accuracy : 0.1996\n",
      "Epochs : 7 , Loss : 2.1930 , Accuracy : 0.2346\n",
      "Epochs : 8 , Loss : 2.1817 , Accuracy : 0.2346\n",
      "Epochs : 9 , Loss : 2.1924 , Accuracy : 0.2018\n",
      "Epochs : 10 , Loss : 2.1737 , Accuracy : 0.2171\n",
      "Epochs : 11 , Loss : 2.1621 , Accuracy : 0.2412\n",
      "Epochs : 12 , Loss : 2.1530 , Accuracy : 0.2303\n",
      "Epochs : 13 , Loss : 2.1682 , Accuracy : 0.2083\n",
      "Epochs : 14 , Loss : 2.1421 , Accuracy : 0.2215\n",
      "Epochs : 15 , Loss : 2.1597 , Accuracy : 0.2149\n",
      "Epochs : 16 , Loss : 2.1646 , Accuracy : 0.2018\n",
      "Epochs : 17 , Loss : 2.1476 , Accuracy : 0.2259\n",
      "Epochs : 18 , Loss : 2.1488 , Accuracy : 0.2281\n",
      "Epochs : 19 , Loss : 2.1491 , Accuracy : 0.2237\n",
      "Epochs : 20 , Loss : 2.1228 , Accuracy : 0.2303\n",
      "Epochs : 21 , Loss : 2.1465 , Accuracy : 0.2215\n",
      "Epochs : 22 , Loss : 2.1070 , Accuracy : 0.2566\n",
      "Epochs : 23 , Loss : 2.1422 , Accuracy : 0.2456\n",
      "Epochs : 24 , Loss : 2.1212 , Accuracy : 0.2654\n",
      "Epochs : 25 , Loss : 2.1467 , Accuracy : 0.2215\n",
      "Epochs : 26 , Loss : 2.1234 , Accuracy : 0.2303\n",
      "Epochs : 27 , Loss : 2.1161 , Accuracy : 0.2237\n",
      "Epochs : 28 , Loss : 2.1214 , Accuracy : 0.2412\n",
      "Epochs : 29 , Loss : 2.1112 , Accuracy : 0.2259\n",
      "Epochs : 30 , Loss : 2.0951 , Accuracy : 0.2368\n",
      "Epochs : 31 , Loss : 2.1220 , Accuracy : 0.2412\n",
      "Epochs : 32 , Loss : 2.1036 , Accuracy : 0.2390\n",
      "Epochs : 33 , Loss : 2.1279 , Accuracy : 0.2325\n",
      "Epochs : 34 , Loss : 2.1114 , Accuracy : 0.2632\n",
      "Epochs : 35 , Loss : 2.0842 , Accuracy : 0.2719\n",
      "Epochs : 36 , Loss : 2.0968 , Accuracy : 0.2478\n",
      "Epochs : 37 , Loss : 2.0902 , Accuracy : 0.2610\n",
      "Epochs : 38 , Loss : 2.0958 , Accuracy : 0.2390\n",
      "Epochs : 39 , Loss : 2.1205 , Accuracy : 0.2544\n",
      "Epochs : 40 , Loss : 2.1351 , Accuracy : 0.2325\n",
      "Epochs : 41 , Loss : 2.1122 , Accuracy : 0.2719\n",
      "Epochs : 42 , Loss : 2.0845 , Accuracy : 0.2785\n",
      "Epochs : 43 , Loss : 2.0950 , Accuracy : 0.2697\n",
      "Epochs : 44 , Loss : 2.0846 , Accuracy : 0.2785\n",
      "Epochs : 45 , Loss : 2.1232 , Accuracy : 0.2303\n",
      "Epochs : 46 , Loss : 2.0829 , Accuracy : 0.2566\n",
      "Epochs : 47 , Loss : 2.0925 , Accuracy : 0.2456\n",
      "Epochs : 48 , Loss : 2.0822 , Accuracy : 0.2675\n",
      "Epochs : 49 , Loss : 2.1302 , Accuracy : 0.2544\n",
      "Epochs : 50 , Loss : 2.0886 , Accuracy : 0.2719\n",
      "Epochs : 51 , Loss : 2.0918 , Accuracy : 0.2588\n",
      "Epochs : 52 , Loss : 2.1072 , Accuracy : 0.2478\n",
      "Epochs : 53 , Loss : 2.0664 , Accuracy : 0.2456\n",
      "Epochs : 54 , Loss : 2.1066 , Accuracy : 0.2566\n",
      "Epochs : 55 , Loss : 2.1164 , Accuracy : 0.2566\n",
      "Epochs : 56 , Loss : 2.0936 , Accuracy : 0.2368\n",
      "Epochs : 57 , Loss : 2.0825 , Accuracy : 0.2566\n",
      "Epochs : 58 , Loss : 2.0672 , Accuracy : 0.2346\n",
      "Epochs : 59 , Loss : 2.0271 , Accuracy : 0.2851\n",
      "Epochs : 60 , Loss : 2.0731 , Accuracy : 0.2566\n",
      "Epochs : 61 , Loss : 2.0694 , Accuracy : 0.2632\n",
      "Epochs : 62 , Loss : 2.0739 , Accuracy : 0.2785\n",
      "Epochs : 63 , Loss : 2.1051 , Accuracy : 0.2763\n",
      "Epochs : 64 , Loss : 2.0665 , Accuracy : 0.2675\n",
      "Epochs : 65 , Loss : 2.0605 , Accuracy : 0.2719\n",
      "Epochs : 66 , Loss : 2.0720 , Accuracy : 0.2654\n",
      "Epochs : 67 , Loss : 2.0931 , Accuracy : 0.2675\n",
      "Epochs : 68 , Loss : 2.0738 , Accuracy : 0.2500\n",
      "Epochs : 69 , Loss : 2.0789 , Accuracy : 0.2829\n",
      "Epochs : 70 , Loss : 2.0804 , Accuracy : 0.2741\n",
      "Epochs : 71 , Loss : 2.1198 , Accuracy : 0.2807\n",
      "Epochs : 72 , Loss : 2.0810 , Accuracy : 0.2588\n",
      "Epochs : 73 , Loss : 2.1028 , Accuracy : 0.2697\n",
      "Epochs : 74 , Loss : 2.0963 , Accuracy : 0.2610\n",
      "Epochs : 75 , Loss : 2.0958 , Accuracy : 0.2763\n",
      "Epochs : 76 , Loss : 2.0658 , Accuracy : 0.2982\n",
      "Epochs : 77 , Loss : 2.0799 , Accuracy : 0.2632\n",
      "Epochs : 78 , Loss : 2.0726 , Accuracy : 0.2785\n",
      "Epochs : 79 , Loss : 2.0616 , Accuracy : 0.2851\n",
      "Epochs : 80 , Loss : 2.0348 , Accuracy : 0.2807\n",
      "Epochs : 81 , Loss : 2.0586 , Accuracy : 0.2632\n",
      "Epochs : 82 , Loss : 2.0816 , Accuracy : 0.2434\n",
      "Epochs : 83 , Loss : 2.0895 , Accuracy : 0.2741\n",
      "Epochs : 84 , Loss : 2.0643 , Accuracy : 0.2610\n",
      "Epochs : 85 , Loss : 2.0955 , Accuracy : 0.2566\n",
      "Epochs : 86 , Loss : 2.0864 , Accuracy : 0.2500\n",
      "Epochs : 87 , Loss : 2.0659 , Accuracy : 0.2632\n",
      "Epochs : 88 , Loss : 2.1142 , Accuracy : 0.2259\n",
      "Epochs : 89 , Loss : 2.0289 , Accuracy : 0.3070\n",
      "Epochs : 90 , Loss : 2.0883 , Accuracy : 0.2544\n",
      "Epochs : 91 , Loss : 2.0754 , Accuracy : 0.2719\n",
      "Epochs : 92 , Loss : 2.1238 , Accuracy : 0.2566\n",
      "Epochs : 93 , Loss : 2.0883 , Accuracy : 0.2566\n",
      "Epochs : 94 , Loss : 2.0829 , Accuracy : 0.2785\n",
      "Epochs : 95 , Loss : 2.0706 , Accuracy : 0.2675\n",
      "Epochs : 96 , Loss : 2.0430 , Accuracy : 0.2917\n",
      "Epochs : 97 , Loss : 2.0750 , Accuracy : 0.2632\n",
      "Epochs : 98 , Loss : 2.0650 , Accuracy : 0.2719\n",
      "Epochs : 99 , Loss : 2.0575 , Accuracy : 0.2500\n",
      "Epochs : 100 , Loss : 2.0321 , Accuracy : 0.2961\n"
     ]
    }
   ],
   "source": [
    "fit(cif_model , 100, train_dl , loss_func , optim , metric = 'accuracy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Boom! I know the accuracy is bad, but for now its okay!\n",
    "\n",
    "We won't be doing the validation and the testing in this notebook, just for simplicity purpose!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch_env",
   "language": "python",
   "name": "pytorch_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
