{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1411c972",
   "metadata": {},
   "source": [
    "### Transformers 103: Using Transformers to generate Shakespeare\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a93f9ae6",
   "metadata": {},
   "source": [
    "This is the last notebook in this series of Transformer notebook. In the previous Notebooks we have already understood and developed in code a perfect understanding of the Transformer Network. In this notebook, we are just going to use the `Transformer` module class we build using Pytorch to train and generate Shakespeare like text.\n",
    "\n",
    "The dataset that we are going to use in this notebook is [Tiny Shakespeare](https://github.com/karpathy/char-rnn/blob/master/data/tinyshakespeare/input.txt).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45d61f1b",
   "metadata": {},
   "source": [
    "Lets get started with the simple imports and get on with our work.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "190a595d",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Importing necessary packages ##\n",
    "\n",
    "import torch\n",
    "import torch.nn\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "\n",
    "from transformer import Transformer\n",
    "\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "\n",
    "## For displaying exact values  and not in exponentiations##\n",
    "torch.set_printoptions(sci_mode=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4ebfcb8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Setting the necessary Hyperparameters ##\n",
    "\n",
    "SEED = 97  # for reproducing each and every result\n",
    "\n",
    "# Specific to data\n",
    "SEQUENCE_LENGTH = 128  # (also needed in model)\n",
    "TRAIN_VAL_TEST_SPLIT = [0.9, 0.05, 0.05]\n",
    "BATCH_SIZE = 64\n",
    "\n",
    "# Specific to model\n",
    "EMBEDDING_DIM = 32\n",
    "NUM_HEADS = 8\n",
    "USE_ENCODER = False\n",
    "USE_DECODER = True\n",
    "DECODER_NUM_LAYERS = 6\n",
    "\n",
    "## Setting the device ##\n",
    "DEVICE = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "\n",
    "# Training\n",
    "LEARNING_RATE = 1e-3\n",
    "NUM_EPOCHS = 10\n",
    "DROPOUT_RATE = 0.1\n",
    "\n",
    "# Saving\n",
    "CKPT = 'checkpoint/bst_transformer.pt'\n",
    "LOAD_MODEL = False\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f9038ea",
   "metadata": {},
   "source": [
    "In this work we are going to develop a character level generator model with the data we have. So, without further adieu lets load our data and make our vocabulary.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89ecd823",
   "metadata": {},
   "source": [
    "#### Reading Data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f0d8fdaa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset length is : 1115394\n"
     ]
    }
   ],
   "source": [
    "## Reading data ##\n",
    "\n",
    "with open(\"tiny_shakespeare.txt\", \"r\") as file:\n",
    "    data = file.read()\n",
    "\n",
    "## Splitting it into a character level list ##\n",
    "data = list(data)\n",
    "\n",
    "print(f\"Dataset length is : {len(data)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2e8cd86",
   "metadata": {},
   "source": [
    "Nice... So, we have a long sequence of all the words broken down to its characters in the dataset.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d632e28",
   "metadata": {},
   "source": [
    "We will update this more in the next few steps. But at first let us build our vocabulary.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5eca00be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lenght of the vocabulary : 65 and the vocabulary is : ['\\n', ' ', '!', '$', '&', \"'\", ',', '-', '.', '3', ':', ';', '?', 'A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z']\n"
     ]
    }
   ],
   "source": [
    "## Vocabulary ##\n",
    "\n",
    "vocab = sorted(list(set(data)))\n",
    "\n",
    "print(f\"Lenght of the vocabulary : {len(vocab)} and the vocabulary is : {vocab}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dbe8d59",
   "metadata": {},
   "source": [
    "Amazing.. So we have loaded our data.\n",
    "\n",
    "Much like the transformers_101 notebook, we need to build a mapping from the characters to the index and vice versa to help the model receive numeric data and convert from numeric data to characters during generation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0f8986e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For sanity check:\n",
      "-------------------\n",
      "\n",
      "char_2_idx :\n",
      "{'\\n': 0, ' ': 1, '!': 2, '$': 3, '&': 4, \"'\": 5, ',': 6, '-': 7, '.': 8, '3': 9, ':': 10, ';': 11, '?': 12, 'A': 13, 'B': 14, 'C': 15, 'D': 16, 'E': 17, 'F': 18, 'G': 19, 'H': 20, 'I': 21, 'J': 22, 'K': 23, 'L': 24, 'M': 25, 'N': 26, 'O': 27, 'P': 28, 'Q': 29, 'R': 30, 'S': 31, 'T': 32, 'U': 33, 'V': 34, 'W': 35, 'X': 36, 'Y': 37, 'Z': 38, 'a': 39, 'b': 40, 'c': 41, 'd': 42, 'e': 43, 'f': 44, 'g': 45, 'h': 46, 'i': 47, 'j': 48, 'k': 49, 'l': 50, 'm': 51, 'n': 52, 'o': 53, 'p': 54, 'q': 55, 'r': 56, 's': 57, 't': 58, 'u': 59, 'v': 60, 'w': 61, 'x': 62, 'y': 63, 'z': 64}\n",
      "-------------------\n",
      "idx_2_char:\n",
      "{0: '\\n', 1: ' ', 2: '!', 3: '$', 4: '&', 5: \"'\", 6: ',', 7: '-', 8: '.', 9: '3', 10: ':', 11: ';', 12: '?', 13: 'A', 14: 'B', 15: 'C', 16: 'D', 17: 'E', 18: 'F', 19: 'G', 20: 'H', 21: 'I', 22: 'J', 23: 'K', 24: 'L', 25: 'M', 26: 'N', 27: 'O', 28: 'P', 29: 'Q', 30: 'R', 31: 'S', 32: 'T', 33: 'U', 34: 'V', 35: 'W', 36: 'X', 37: 'Y', 38: 'Z', 39: 'a', 40: 'b', 41: 'c', 42: 'd', 43: 'e', 44: 'f', 45: 'g', 46: 'h', 47: 'i', 48: 'j', 49: 'k', 50: 'l', 51: 'm', 52: 'n', 53: 'o', 54: 'p', 55: 'q', 56: 'r', 57: 's', 58: 't', 59: 'u', 60: 'v', 61: 'w', 62: 'x', 63: 'y', 64: 'z'}\n"
     ]
    }
   ],
   "source": [
    "## Mapping ##\n",
    "\n",
    "char_2_idx = {ch: idx for idx, ch in enumerate(vocab)}\n",
    "\n",
    "idx_2_char = {idx: ch for idx, ch in enumerate(vocab)}\n",
    "\n",
    "print(\n",
    "    f\"For sanity check:\\n-------------------\\n\\nchar_2_idx :\\n{char_2_idx}\\n-------------------\\nidx_2_char:\\n{idx_2_char}\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebded815",
   "metadata": {},
   "source": [
    "Look perfect.\n",
    "\n",
    "Now its time that we build our Pytorch dataset and dataloader.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f998d512",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Dataset ##\n",
    "\n",
    "\n",
    "class ShakespeareDataset(Dataset):\n",
    "    \"\"\"Builds the Shakespeare dataset.\"\"\"\n",
    "\n",
    "    def __init__(self, data, sequence_length):\n",
    "        \"\"\"Constructor.\"\"\"\n",
    "\n",
    "        self.data = data\n",
    "        self.sequence_length = sequence_length\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"Returns a single sample, i.e., a sequence and label.\"\"\"\n",
    "        sequence = [\n",
    "            char_2_idx[ch]\n",
    "            for ch in self.data[\n",
    "                idx : idx + self.sequence_length\n",
    "            ]\n",
    "        ]\n",
    "        label = [\n",
    "            char_2_idx[ch]\n",
    "            for ch in self.data[\n",
    "                idx + 1 : idx + 1 + self.sequence_length\n",
    "            ]\n",
    "        ]\n",
    "\n",
    "        return torch.tensor(sequence), torch.tensor(label)\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"Length of the dataset\"\"\"\n",
    "        return len(self.data) - (self.sequence_length + 1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa817178",
   "metadata": {},
   "source": [
    "Let's just do some sanity checking on the dataset.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c49585f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset length is 1115265\n"
     ]
    }
   ],
   "source": [
    "## Sanity checking ##\n",
    "\n",
    "shakespeare_dataset = ShakespeareDataset(data=data, sequence_length=SEQUENCE_LENGTH)\n",
    "\n",
    "print(f\"Dataset length is {len(shakespeare_dataset)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8505a6d3",
   "metadata": {},
   "source": [
    "Now before doing anything we must split the dataset into train, val, and test.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9caa2545",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train dataset length : 1003739\n",
      "Validation dataset length : 55763\n",
      "Test dataset length : 55763\n"
     ]
    }
   ],
   "source": [
    "## Splitting the shakespeare dataset into train, validation and test ##\n",
    "\n",
    "train_dataset, val_dataset, test_dataset = random_split(\n",
    "    shakespeare_dataset,\n",
    "    lengths=TRAIN_VAL_TEST_SPLIT,\n",
    "    generator=torch.Generator().manual_seed(SEED),\n",
    ")\n",
    "\n",
    "print(f\"Train dataset length : {len(train_dataset)}\")\n",
    "print(f\"Validation dataset length : {len(val_dataset)}\")\n",
    "print(f\"Test dataset length : {len(val_dataset)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9800451d",
   "metadata": {},
   "source": [
    "Perfect!! This looks awesome!!\n",
    "\n",
    "Now we can poke our heads once inside the training dataset and see if it actually is giving what we want.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bbbb23c",
   "metadata": {},
   "source": [
    "For this we are going to use a utility function.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2ab85be9",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Utility function to show data ##\n",
    "\n",
    "\n",
    "def show_data(data, verbose=True):\n",
    "    \"\"\"Given a data tensor, maps them to string and prints them.\"\"\"\n",
    "    str_data = [idx_2_char[each_char.item()] for each_char in data.data]\n",
    "    if verbose:\n",
    "        print(str_data)\n",
    "    else:\n",
    "        return str_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9971df08",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequence\n",
      "----------\n",
      "['\\n', 'A', 'n', 'd', ' ', 'p', 'i', 't', 'c', 'h', ' ', 'o', 'u', 'r', ' ', 'e', 'v', 'i', 'l', 's', ' ', 't', 'h', 'e', 'r', 'e', '?', ' ', 'O', ',', ' ', 'f', 'i', 'e', ',', ' ', 'f', 'i', 'e', ',', ' ', 'f', 'i', 'e', '!', '\\n', 'W', 'h', 'a', 't', ' ', 'd', 'o', 's', 't', ' ', 't', 'h', 'o', 'u', ',', ' ', 'o', 'r', ' ', 'w', 'h', 'a', 't', ' ', 'a', 'r', 't', ' ', 't', 'h', 'o', 'u', ',', ' ', 'A', 'n', 'g', 'e', 'l', 'o', '?', '\\n', 'D', 'o', 's', 't', ' ', 't', 'h', 'o', 'u', ' ', 'd', 'e', 's', 'i', 'r', 'e', ' ', 'h', 'e', 'r', ' ', 'f', 'o', 'u', 'l', 'l', 'y', ' ', 'f', 'o', 'r', ' ', 't', 'h', 'o', 's', 'e', ' ', 't', 'h']\n",
      "The shape of the sequence is : torch.Size([128])\n",
      "\n",
      "Label\n",
      "----------\n",
      "['A', 'n', 'd', ' ', 'p', 'i', 't', 'c', 'h', ' ', 'o', 'u', 'r', ' ', 'e', 'v', 'i', 'l', 's', ' ', 't', 'h', 'e', 'r', 'e', '?', ' ', 'O', ',', ' ', 'f', 'i', 'e', ',', ' ', 'f', 'i', 'e', ',', ' ', 'f', 'i', 'e', '!', '\\n', 'W', 'h', 'a', 't', ' ', 'd', 'o', 's', 't', ' ', 't', 'h', 'o', 'u', ',', ' ', 'o', 'r', ' ', 'w', 'h', 'a', 't', ' ', 'a', 'r', 't', ' ', 't', 'h', 'o', 'u', ',', ' ', 'A', 'n', 'g', 'e', 'l', 'o', '?', '\\n', 'D', 'o', 's', 't', ' ', 't', 'h', 'o', 'u', ' ', 'd', 'e', 's', 'i', 'r', 'e', ' ', 'h', 'e', 'r', ' ', 'f', 'o', 'u', 'l', 'l', 'y', ' ', 'f', 'o', 'r', ' ', 't', 'h', 'o', 's', 'e', ' ', 't', 'h', 'i']\n",
      "The shape of the label is : torch.Size([128])\n"
     ]
    }
   ],
   "source": [
    "## Testing ##\n",
    "\n",
    "torch.manual_seed(SEED)\n",
    "\n",
    "random_idx = torch.randint(low=0, high=len(train_dataset), size=(1,))\n",
    "\n",
    "sequence, label = train_dataset[random_idx]\n",
    "\n",
    "print(f\"Sequence\")\n",
    "print(f\"----------\")\n",
    "print(f\"{show_data(sequence, False)}\")\n",
    "print(f\"The shape of the sequence is : {sequence.shape}\")\n",
    "\n",
    "print(f\"\\nLabel\")\n",
    "print(f\"----------\")\n",
    "print(f\"{show_data(label, False)}\")\n",
    "print(f\"The shape of the label is : {label.shape}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bae71246",
   "metadata": {},
   "source": [
    "Perfect...\n",
    "\n",
    "Now its time that we setup our DataLoaders.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0b81bd1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Dataloader length : 15684\n",
      "Val Dataloader length : 872\n",
      "Test Dataloader length : 872\n"
     ]
    }
   ],
   "source": [
    "## Setting up dataloaders ##\n",
    "\n",
    "train_dl = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "val_dl = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "test_dl = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "\n",
    "print(f\"Train Dataloader length : {len(train_dl)}\")\n",
    "print(f\"Val Dataloader length : {len(val_dl)}\")\n",
    "print(f\"Test Dataloader length : {len(test_dl)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8daab8d6",
   "metadata": {},
   "source": [
    "Nice... So we are almost done with everything regarding the dataset and dataloading. Now its time to setup our model. To do this we are going to use our already built Transformer module from the previous notebook.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "416c0c45",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Setting our transformer model ##\n",
    "\n",
    "transformer = Transformer(\n",
    "    vocab_size=len(vocab),\n",
    "    sequence_length=SEQUENCE_LENGTH,\n",
    "    d_embed=EMBEDDING_DIM,\n",
    "    use_encoder=USE_ENCODER,\n",
    "    use_decoder=USE_DECODER,\n",
    "    num_heads=NUM_HEADS,\n",
    "    decoder_num_layers=DECODER_NUM_LAYERS,\n",
    "    device=DEVICE,\n",
    "    dropout_rate=DROPOUT_RATE\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6c83f59a",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Setting the entire transformer to default device ##\n",
    "\n",
    "transformer = transformer.to(DEVICE)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5afb903",
   "metadata": {},
   "source": [
    "Now that we have nicely setup our transformer model, its time to setup the loss function and the optimizer. We are going to use the Adam Optimizer and the CrossEntropyLoss function.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "38115531",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Setting up loss function ##\n",
    "\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "## Setting up optimizer ##\n",
    "\n",
    "optim = torch.optim.Adam(transformer.parameters(), lr=LEARNING_RATE)\n",
    "\n",
    "## Learning Rate scheduler ##\n",
    "\n",
    "#scheduler = torch.optim.lr_scheduler.CyclicLR(optim, base_lr=1e-4, max_lr=1e-2, cycle_momentum=False)\n",
    "\n",
    "## Keeping the epoch losses ##\n",
    "epoch_train_losses = []\n",
    "epoch_val_losses = []\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c548243d",
   "metadata": {},
   "source": [
    "Now we are going to setup a small utility function to derive the single sweep loss function. This is necessary to check the validation loss from time to time. Also at the end it would come in handy to see how our model works out for test data too.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d4107e44",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Single sweep loss function ##\n",
    "\n",
    "\n",
    "def single_sweep_loss(model, dataloader, device=DEVICE):\n",
    "    \"\"\"Single sweep loss function\"\"\"\n",
    "\n",
    "    minibatch_losses = []\n",
    "    with torch.no_grad():\n",
    "        for sequence, label in dataloader:\n",
    "            sequence = sequence.to(device)\n",
    "            label = label.to(device)\n",
    "            pred = model(outputs=sequence)\n",
    "            B, L, C = pred.shape\n",
    "            loss = criterion(pred.view(B * L, C), label.view(B * L))\n",
    "            minibatch_losses.append(loss.item())\n",
    "\n",
    "    return sum(minibatch_losses) / len(minibatch_losses)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d675964",
   "metadata": {},
   "source": [
    "Nicely setup....\n",
    "\n",
    "Now it is time to train our model for atleast 50 epochs and lets see how it performs.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b1879772",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch : 1 / 10 ::: 100%|██████████| 15684/15684 [50:08<00:00,  5.21it/s, loss=1.66]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Doing single sweep loss calculation\n",
      "Best Validation Loss : 1.5737870949397392\n",
      "Saving model now...\n",
      "Epoch : 1 / 10 ::: Training Loss : 1.5741180489331414 , Validation Loss : 1.5737870949397392\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch : 2 / 10 ::: 100%|██████████| 15684/15684 [50:47<00:00,  5.15it/s, loss=1.6]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Doing single sweep loss calculation\n",
      "Best Validation Loss : 1.5285188949436224\n",
      "Saving model now...\n",
      "Epoch : 2 / 10 ::: Training Loss : 1.528699892496339 , Validation Loss : 1.5285188949436224\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch : 3 / 10 ::: 100%|██████████| 15684/15684 [51:17<00:00,  5.10it/s, loss=1.57]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Doing single sweep loss calculation\n",
      "Best Validation Loss : 1.5013233946824291\n",
      "Saving model now...\n",
      "Epoch : 3 / 10 ::: Training Loss : 1.5013290766139202 , Validation Loss : 1.5013233946824291\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch : 4 / 10 ::: 100%|██████████| 15684/15684 [51:56<00:00,  5.03it/s, loss=1.61]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Doing single sweep loss calculation\n",
      "Best Validation Loss : 1.4826875184107264\n",
      "Saving model now...\n",
      "Epoch : 4 / 10 ::: Training Loss : 1.4826288189139485 , Validation Loss : 1.4826875184107264\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch : 5 / 10 ::: 100%|██████████| 15684/15684 [51:50<00:00,  5.04it/s, loss=1.58]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Doing single sweep loss calculation\n",
      "Best Validation Loss : 1.4727591244726006\n",
      "Saving model now...\n",
      "Epoch : 5 / 10 ::: Training Loss : 1.473019695212056 , Validation Loss : 1.4727591244726006\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch : 6 / 10 ::: 100%|██████████| 15684/15684 [51:50<00:00,  5.04it/s, loss=1.61]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Doing single sweep loss calculation\n",
      "Best Validation Loss : 1.4650515639180437\n",
      "Saving model now...\n",
      "Epoch : 6 / 10 ::: Training Loss : 1.4654447093997431 , Validation Loss : 1.4650515639180437\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch : 7 / 10 ::: 100%|██████████| 15684/15684 [51:44<00:00,  5.05it/s, loss=1.6] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Doing single sweep loss calculation\n",
      "Best Validation Loss : 1.4604199184190243\n",
      "Saving model now...\n",
      "Epoch : 7 / 10 ::: Training Loss : 1.4607035502491177 , Validation Loss : 1.4604199184190243\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch : 8 / 10 ::: 100%|██████████| 15684/15684 [51:43<00:00,  5.05it/s, loss=1.57]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Doing single sweep loss calculation\n",
      "Best Validation Loss : 1.4540711500502508\n",
      "Saving model now...\n",
      "Epoch : 8 / 10 ::: Training Loss : 1.4543130292222988 , Validation Loss : 1.4540711500502508\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch : 9 / 10 ::: 100%|██████████| 15684/15684 [51:51<00:00,  5.04it/s, loss=1.62]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Doing single sweep loss calculation\n",
      "Best Validation Loss : 1.4481872492427126\n",
      "Saving model now...\n",
      "Epoch : 9 / 10 ::: Training Loss : 1.4485138261448938 , Validation Loss : 1.4481872492427126\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch : 10 / 10 ::: 100%|██████████| 15684/15684 [51:52<00:00,  5.04it/s, loss=1.5] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Doing single sweep loss calculation\n",
      "Best Validation Loss : 1.4456292814617857\n",
      "Saving model now...\n",
      "Epoch : 10 / 10 ::: Training Loss : 1.4458694900520235 , Validation Loss : 1.4456292814617857\n"
     ]
    }
   ],
   "source": [
    "best_val_loss = float(\"inf\")\n",
    "\n",
    "## Training our model ##\n",
    "\n",
    "if LOAD_MODEL:\n",
    "    try:\n",
    "        transformer.load_state_dict(torch.load(CKPT))\n",
    "    except:\n",
    "        print(f\"Check the model checkpoint path!\")\n",
    "\n",
    "for i in range(NUM_EPOCHS):\n",
    "    loop = tqdm(train_dl)\n",
    "\n",
    "    transformer.train()\n",
    "\n",
    "    for sequence, label in loop:\n",
    "        sequence = sequence.to(DEVICE)\n",
    "        label = label.to(DEVICE)\n",
    "        pred = transformer(outputs=sequence)\n",
    "        B, L, C = pred.shape\n",
    "\n",
    "        loss = criterion(pred.view(B * L, C), label.view(B * L))\n",
    "\n",
    "        loop.set_description(f\"Epoch : {i + 1} / {NUM_EPOCHS} ::\")\n",
    "        loop.set_postfix(loss=loss.item())\n",
    "\n",
    "        optim.zero_grad()\n",
    "        loss.backward()\n",
    "        optim.step()\n",
    "        # scheduler.step()\n",
    "\n",
    "    print(f\"Doing single sweep loss calculation\")\n",
    "\n",
    "    transformer.eval()\n",
    "\n",
    "    train_loss = single_sweep_loss(transformer, train_dl)\n",
    "    val_loss = single_sweep_loss(transformer, val_dl)\n",
    "\n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        print(f\"Best Validation Loss : {best_val_loss}\")\n",
    "        print(f\"Saving model now...\")\n",
    "        torch.save(transformer.state_dict(), CKPT)\n",
    "\n",
    "    print(\n",
    "        f\"Epoch : {i + 1} / {NUM_EPOCHS} ::: Training Loss : {train_loss} , Validation Loss : {val_loss}\"\n",
    "    )\n",
    "\n",
    "    epoch_train_losses.append(train_loss)\n",
    "    epoch_val_losses.append(val_loss)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "9940711f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tell to have the yet puner me speaking,\n",
      "That away your kingland: tray a sorring'd and a tyrat's.\n",
      "\n",
      "CLOMENES:\n",
      "I prove, I happy one, and we can; more!\n",
      "\n",
      "MENENIUS:\n",
      "You cannot my business ance, stand why hour,\n",
      "Would sit me as you will: my queen spirike,\n",
      "For what nather of by the lighterity,\n",
      "And for this poison; in the dispose defend;\n",
      "And not mother shall strange to dost maid\n",
      "Do faith the king!\n",
      "\n",
      "GLOUCESTER:\n",
      "Tyrrachignce to galise.\n",
      "\n",
      "ANTONIO:\n",
      "Ox he not drook and was but our oddness.\n",
      "\n",
      "LEONTES:\n",
      "Would fear to spake of her winderful hell be\n",
      "Romeo, dield by his manner guilt, beingmon,\n",
      "That changely repeaty my defut.\n",
      "\n",
      "JULIET,\n",
      "Thou lettheren's I will with it, to near. as thou leave.\n",
      "\n",
      "QUEEN MARGARET:\n",
      "You\n"
     ]
    }
   ],
   "source": [
    "## Generating ##\n",
    "\n",
    "transformer.load_state_dict(torch.load(CKPT))\n",
    "\n",
    "transformer.eval()\n",
    "\n",
    "transformer.generate(max_length=696, idx_2_char_map=idx_2_char, device=DEVICE)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch_env",
   "language": "python",
   "name": "pytorch_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
