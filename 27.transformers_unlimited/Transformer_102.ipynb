{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "28b449f0",
   "metadata": {},
   "source": [
    "### Transformers 102: Building entire Transformer Architetcure ###"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eba2fac7",
   "metadata": {},
   "source": [
    "This notebook is built on top of the \"Transformers 101\" notebook that I previously created. In the previous notebook, I created a small version of the Transformer Decoder network and it was mostly for understanding withou undertaking much of the computational efficacies and clean code-ups. In this notebook, I will write the entire the code up of the Transformer model with both the encoder and decoder bits. I will also release a python file along with the notebook for dynamic usage later on.\n",
    "\n",
    "So, let's just begin."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c0af3547",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Importing necessary packages ##\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c7907ed",
   "metadata": {},
   "source": [
    "We are going to build everything piece by piece based on the architectural diagram given in this [paper](https://arxiv.org/pdf/1706.03762.pdf)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ff04f43",
   "metadata": {},
   "source": [
    "The first thing we will build up is the **Scaled Dot Product Attention**. This is pretty simple, as we previously saw in our previous notebook. The mechanism deals with the interaction between three matrices, namely, Query, Key and Value. \n",
    "\n",
    "The calculation is very simple : $softmax(\\frac{Q \\cdot K^{T}}{\\sqrt{d_{k}}}) \\cdot V$ .\n",
    "\n",
    "In the previous notebook we implemented the Decoder of the Transformer where we were needed to do a masking of after the dot product of the Query and Key to not allow interaction of sequences from future. But if we build up our encoder too, this masking is no longer necessary. Hence, we would build a generic Scaled Dot Product attention module which allows for optional Masking.\n",
    "\n",
    "So, lets implement this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3c375f0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Scaled Dot Product Attention ##\n",
    "\n",
    "\n",
    "class ScaledDotProductAttention(nn.Module):\n",
    "    \"\"\"Implements the scaled dot product attention.\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        d_embed: int,\n",
    "        d_k: int,\n",
    "        d_v: int = None,\n",
    "        mask: bool = False,\n",
    "    ):\n",
    "        \"\"\"Constructor\"\"\"\n",
    "\n",
    "        super().__init__()\n",
    "\n",
    "        # If d_v is not specified set to same as d_k #\n",
    "        if d_v == None:\n",
    "            d_v = d_k\n",
    "\n",
    "        # Query, key and values linear layers #\n",
    "        self.query_ffn = nn.Linear(d_embed, d_k, bias=False)\n",
    "        self.key_ffn = nn.Linear(d_embed, d_k, bias=False)\n",
    "        self.value_ffn = nn.Linear(d_embed, d_v, bias=False)\n",
    "\n",
    "        self.mask = mask\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        x: torch.tensor,\n",
    "        y: torch.tensor = None,\n",
    "        z: torch.tensor = None,\n",
    "        return_wt: bool = False,\n",
    "    ):\n",
    "        \"\"\"Applies a forward pass through the Scaled Dot Product Attention.\"\"\"\n",
    "\n",
    "        # Getting the key, query and the value\n",
    "        key = self.key_ffn(x)\n",
    "\n",
    "        if y == None:\n",
    "            y = x.clone()\n",
    "\n",
    "        if z == None:\n",
    "            z = x.clone()\n",
    "\n",
    "        query = self.query_ffn(y)\n",
    "        value = self.value_ffn(z)\n",
    "\n",
    "        # Getting d_k from query\n",
    "        d_k = key.shape[-1]\n",
    "\n",
    "        # Calculating weights\n",
    "        weight = query @ key.transpose(-2, -1) / d_k**0.5\n",
    "\n",
    "        # Setting mask\n",
    "        if self.mask:\n",
    "            weight = torch.tril(weight)\n",
    "            weight = weight.masked_fill(weight == 0, float(\"-inf\"))\n",
    "\n",
    "        # Pass through softmax\n",
    "        weight = F.softmax(weight, dim=-1)\n",
    "\n",
    "        # Finally product with the values and return\n",
    "        if return_wt:\n",
    "            return weight @ value, weight\n",
    "\n",
    "        return weight @ value\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38bc6119",
   "metadata": {},
   "source": [
    "One thing that is worth noting is that in the forward pass, I optionally gave an opportunity to send in 3 input tensors which can be individually used for generating the keys, queries and values. This makes it highly generalizable since now we can use this exact class for cross-attention as well as self-attention. Moreover, I also allowed for an optional Mask term which makes the module generic for Masked attention as well as unmasked attention."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e204251",
   "metadata": {},
   "source": [
    "Lets test it out..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1202095d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Without Mask\n",
      "--------------\n",
      "Input :\n",
      "\ttensor([[-2.1115, -0.4614],\n",
      "        [ 0.9178,  1.7334]])\n",
      "--------------\n",
      "--------------\n",
      "Output : tensor([[-0.5837, -0.1434],\n",
      "        [-0.6758, -0.1538]], grad_fn=<MmBackward0>)\n",
      "--------------\n",
      "Weights after softmax for sanity check : tensor([[0.3377, 0.6623],\n",
      "        [0.2804, 0.7196]], grad_fn=<SoftmaxBackward0>)\n",
      "\n",
      "\n",
      "With Mask\n",
      "--------------\n",
      "Output : tensor([[-0.8411, -1.1339],\n",
      "        [ 0.0678, -0.2628]], grad_fn=<MmBackward0>)\n",
      "--------------\n",
      "Weights after softmax for sanity check : tensor([[1.0000, 0.0000],\n",
      "        [0.4882, 0.5118]], grad_fn=<SoftmaxBackward0>)\n"
     ]
    }
   ],
   "source": [
    "## Test ScaledDotProductAttention ##\n",
    "\n",
    "## For reproducibility ##\n",
    "torch.manual_seed(97)\n",
    "\n",
    "## Without mask ##\n",
    "print(f'Without Mask')\n",
    "print(f'--------------')\n",
    "test_attn_1 = ScaledDotProductAttention(d_embed=2, d_k=2)\n",
    "\n",
    "x = torch.randn(2,2)\n",
    "\n",
    "print(f'Input :\\n\\t{x}')\n",
    "print(f'--------------')\n",
    "\n",
    "result_1, wt = test_attn_1(x, return_wt=True)\n",
    "\n",
    "print(f'--------------')\n",
    "print(f'Output : {result_1}')\n",
    "\n",
    "print(f'--------------')\n",
    "print(f'Weights after softmax for sanity check : {wt}')\n",
    "\n",
    "print(f'\\n\\nWith Mask')\n",
    "print(f'--------------')\n",
    "test_attn_2 = ScaledDotProductAttention(d_embed=2, d_k=2,mask=True)\n",
    "\n",
    "result_2, wt = test_attn_2(x, return_wt=True)\n",
    "\n",
    "print(f'Output : {result_2}')\n",
    "\n",
    "print(f'--------------')\n",
    "print(f'Weights after softmax for sanity check : {wt}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abdbb40b",
   "metadata": {},
   "source": [
    "Now we are done with the simple building block of our Transformer model, we can setup the other core bit of it, which is the **Multi-head Attention** module."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5ff2b702",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    \"\"\"Implements the Multihead Attention.\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        d_embed: int,\n",
    "        num_heads: int,\n",
    "        d_k: int = None,\n",
    "        mask: bool = False,\n",
    "    ):\n",
    "        \"\"\"Constructor\"\"\"\n",
    "\n",
    "        super().__init__()\n",
    "\n",
    "        d_v = d_embed // num_heads\n",
    "        if d_k == None:\n",
    "            d_k = d_v\n",
    "\n",
    "        self.multi_heads = nn.ModuleList(\n",
    "            [\n",
    "                ScaledDotProductAttention(\n",
    "                    d_embed=d_embed,\n",
    "                    d_k=d_k,\n",
    "                    d_v=d_v,\n",
    "                    mask=mask,\n",
    "                )\n",
    "                for _ in range(num_heads)\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        self.mask = mask\n",
    "\n",
    "    def forward(self, x: torch.tensor, y: torch.tensor = None, z: torch.tensor = None):\n",
    "        \"\"\"Forward Pass\"\"\"\n",
    "\n",
    "        return torch.cat([head(x, y, z) for head in self.multi_heads], dim=-1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "eaa4f8a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output shape is : torch.Size([4, 8, 64])\n",
      "Type of mult_attn is <class '__main__.MultiHeadAttention'>\n"
     ]
    }
   ],
   "source": [
    "## Testing ##\n",
    "\n",
    "## For reproducibility ##\n",
    "torch.manual_seed(97)\n",
    "\n",
    "x = torch.randn(4, 8, 64)\n",
    "\n",
    "mult_attn = MultiHeadAttention(d_embed=64, num_heads=8)\n",
    "\n",
    "print(f'Output shape is : {mult_attn(x).shape}')\n",
    "\n",
    "#print(f'Multihead attention module :\\n{mult_attn}')\n",
    "\n",
    "print(f'Type of mult_attn is {type(mult_attn)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b27b896e",
   "metadata": {},
   "source": [
    "Looks perfect!! Now moving on..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81ea6a47",
   "metadata": {},
   "source": [
    "In the encoder and the decoder block, after the Attention layers, there is a simple Feedforward network which maps the output of the attention mechanism to a richer representation. This sub-module is called **Position Wise Feed Forward Networks**. It basically does the following thing.\n",
    "\n",
    "1. Pass the output of the attention through a linear layer with increased features. (In the original paper it is 4 x d_model).\n",
    "2. Pass it through a ReLU non-linearity.\n",
    "3. Finally pass it through another linear layer with reduced features (map it back to d_model)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5bce994a",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Position wise Feed Forward Networks ##\n",
    "\n",
    "class PositionWiseFFN(nn.Module):\n",
    "    \"\"\"Implements the Position Wise Feed Forward Networks\"\"\"\n",
    "    \n",
    "    def __init__(self, d_embed : int):\n",
    "        \"\"\"Constructor\"\"\"\n",
    "        \n",
    "        super().__init__()\n",
    "        \n",
    "        self.pwffn = nn.Sequential(\n",
    "            nn.Linear(in_features=d_embed, out_features=4*d_embed, bias=False),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(in_features=4*d_embed, out_features=d_embed, bias=False),\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \"\"\"Forward Pass\"\"\"\n",
    "        \n",
    "        return self.pwffn(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a8cc684c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output shape : torch.Size([4, 8, 512])\n"
     ]
    }
   ],
   "source": [
    "## testing ##\n",
    "\n",
    "## For reproducibility ##\n",
    "torch.manual_seed(97)\n",
    "\n",
    "pwffn = PositionWiseFFN(512)\n",
    "\n",
    "inp = torch.randn(4, 8 , 512)\n",
    "\n",
    "print(f'Output shape : {pwffn(inp).shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ddabf13",
   "metadata": {},
   "source": [
    "Perfect!!\n",
    "\n",
    "We are slowly building our work."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec5e6695",
   "metadata": {},
   "source": [
    "Next up is the formation of the Position embedding. We are going to use the sinusoidal embedding used by the original paper. Since, it doesn't have any learnable parameters we will implement the simple using a python function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6fc3be25",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Postion Encoding ##\n",
    "\n",
    "def PositionalEncoding(seq_length : int, d_embed : int, device = 'cpu'):\n",
    "    \"\"\"Positional Embedding\"\"\"\n",
    "    \n",
    "    pos = torch.arange(seq_length , device=device).unsqueeze(-1)\n",
    "    i = torch.arange(d_embed, device=device)\n",
    "    \n",
    "    position_embedding = torch.empty((seq_length, d_embed))\n",
    "    \n",
    "    position_embedding[: , ::2] = torch.sin(pos/1000**(i[::2]/d_embed))\n",
    "    position_embedding[: , 1::2] = torch.cos(pos/1000**(i[1::2]/d_embed))\n",
    "    \n",
    "    return position_embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "328897ea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.0000e+00,  1.0000e+00,  0.0000e+00,  1.0000e+00,  0.0000e+00,\n",
       "          1.0000e+00,  0.0000e+00,  1.0000e+00,  0.0000e+00,  1.0000e+00,\n",
       "          0.0000e+00,  1.0000e+00,  0.0000e+00,  1.0000e+00,  0.0000e+00,\n",
       "          1.0000e+00,  0.0000e+00,  1.0000e+00,  0.0000e+00,  1.0000e+00,\n",
       "          0.0000e+00,  1.0000e+00,  0.0000e+00,  1.0000e+00,  0.0000e+00,\n",
       "          1.0000e+00,  0.0000e+00,  1.0000e+00,  0.0000e+00,  1.0000e+00,\n",
       "          0.0000e+00,  1.0000e+00],\n",
       "        [ 8.4147e-01,  6.9250e-01,  6.0469e-01,  8.6618e-01,  4.0931e-01,\n",
       "          9.4281e-01,  2.7043e-01,  9.7575e-01,  1.7689e-01,  9.8975e-01,\n",
       "          1.1522e-01,  9.9567e-01,  7.4919e-02,  9.9817e-01,  4.8678e-02,\n",
       "          9.9923e-01,  3.1618e-02,  9.9968e-01,  2.0534e-02,  9.9986e-01,\n",
       "          1.3335e-02,  9.9994e-01,  8.6595e-03,  9.9998e-01,  5.6234e-03,\n",
       "          9.9999e-01,  3.6517e-03,  1.0000e+00,  2.3714e-03,  1.0000e+00,\n",
       "          1.5399e-03,  1.0000e+00],\n",
       "        [ 9.0930e-01, -4.0877e-02,  9.6323e-01,  5.0052e-01,  7.4690e-01,\n",
       "          7.7780e-01,  5.2071e-01,  9.0418e-01,  3.4821e-01,  9.5921e-01,\n",
       "          2.2891e-01,  9.8273e-01,  1.4942e-01,  9.9271e-01,  9.7240e-02,\n",
       "          9.9692e-01,  6.3203e-02,  9.9870e-01,  4.1059e-02,  9.9945e-01,\n",
       "          2.6667e-02,  9.9977e-01,  1.7318e-02,  9.9990e-01,  1.1247e-02,\n",
       "          9.9996e-01,  7.3034e-03,  9.9998e-01,  4.7427e-03,  9.9999e-01,\n",
       "          3.0798e-03,  1.0000e+00],\n",
       "        [ 1.4112e-01, -7.4912e-01,  9.2964e-01,  8.9891e-04,  9.5363e-01,\n",
       "          5.2382e-01,  7.3219e-01,  7.8875e-01,  5.0854e-01,  9.0901e-01,\n",
       "          3.3955e-01,  9.6128e-01,  2.2308e-01,  9.8361e-01,  1.4557e-01,\n",
       "          9.9308e-01,  9.4726e-02,  9.9708e-01,  6.1567e-02,  9.9877e-01,\n",
       "          3.9995e-02,  9.9948e-01,  2.5976e-02,  9.9978e-01,  1.6869e-02,\n",
       "          9.9991e-01,  1.0955e-02,  9.9996e-01,  7.1141e-03,  9.9998e-01,\n",
       "          4.6198e-03,  9.9999e-01],\n",
       "        [-7.5680e-01, -9.9666e-01,  5.1762e-01, -4.9896e-01,  9.9328e-01,\n",
       "          2.0994e-01,  8.8910e-01,  6.3507e-01,  6.5283e-01,  8.4017e-01,\n",
       "          4.4566e-01,  9.3152e-01,  2.9548e-01,  9.7093e-01,  1.9356e-01,\n",
       "          9.8771e-01,  1.2615e-01,  9.9481e-01,  8.2049e-02,  9.9781e-01,\n",
       "          5.3316e-02,  9.9908e-01,  3.4632e-02,  9.9961e-01,  2.2492e-02,\n",
       "          9.9984e-01,  1.4606e-02,  9.9993e-01,  9.4854e-03,  9.9997e-01,\n",
       "          6.1597e-03,  9.9999e-01],\n",
       "        [-9.5892e-01, -6.3126e-01, -1.0512e-01, -8.6528e-01,  8.5890e-01,\n",
       "         -1.2796e-01,  9.7975e-01,  4.5059e-01,  7.7653e-01,  7.5410e-01,\n",
       "          5.4584e-01,  8.9369e-01,  3.6622e-01,  9.5470e-01,  2.4109e-01,\n",
       "          9.8081e-01,  1.5746e-01,  9.9189e-01,  1.0250e-01,  9.9658e-01,\n",
       "          6.6627e-02,  9.9856e-01,  4.3285e-02,  9.9939e-01,  2.8113e-02,\n",
       "          9.9974e-01,  1.8258e-02,  9.9989e-01,  1.1857e-02,  9.9995e-01,\n",
       "          7.6996e-03,  9.9998e-01],\n",
       "        [-2.7942e-01,  1.2236e-01, -6.8507e-01, -1.0000e+00,  5.7403e-01,\n",
       "         -4.5122e-01,  9.9739e-01,  2.4426e-01,  8.7574e-01,  6.5258e-01,\n",
       "          6.3875e-01,  8.4813e-01,  4.3491e-01,  9.3499e-01,  2.8804e-01,\n",
       "          9.7241e-01,  1.8860e-01,  9.8833e-01,  1.2290e-01,  9.9507e-01,\n",
       "          7.9926e-02,  9.9792e-01,  5.1934e-02,  9.9912e-01,  3.3734e-02,\n",
       "          9.9963e-01,  2.1909e-02,  9.9984e-01,  1.4228e-02,  9.9993e-01,\n",
       "          9.2394e-03,  9.9997e-01],\n",
       "        [ 6.5699e-01,  8.0073e-01, -9.8613e-01, -8.6707e-01,  1.8858e-01,\n",
       "         -7.2287e-01,  9.4070e-01,  2.6080e-02,  9.4733e-01,  5.3768e-01,\n",
       "          7.2315e-01,  7.9524e-01,  5.0115e-01,  9.1186e-01,  3.3431e-01,\n",
       "          9.6251e-01,  2.1956e-01,  9.8413e-01,  1.4325e-01,  9.9330e-01,\n",
       "          9.3211e-02,  9.9717e-01,  6.0580e-02,  9.9881e-01,  3.9354e-02,\n",
       "          9.9950e-01,  2.5559e-02,  9.9979e-01,  1.6599e-02,  9.9991e-01,\n",
       "          1.0779e-02,  9.9996e-01]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Testing ##\n",
    "\n",
    "position_embedding = PositionalEncoding(8,32)\n",
    "\n",
    "position_embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc4b9f83",
   "metadata": {},
   "source": [
    "Again... Looks perfect!!\n",
    "\n",
    "We have almost built up all the necessary modules. The few things that are remaining are the Add and Norm modules present in each of the Encoder and Decoder block layers. These are nothing but a simple residual connection of the input after being passed through a sublayer (like attention or point wise ffn), with an added Dropout and then passed through simple LayerNorm layer.\n",
    "\n",
    "So, our next goal is to prepare this module called the **ResidualDropoutNorm**. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "53386cd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResidualDropoutNorm(nn.Module):\n",
    "    \"\"\"Implements the Residual Dropout Norm Layer.\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self, sublayer: nn.Module, d_embed: int = 512, dropout_rate: float = 0.1\n",
    "    ):\n",
    "        \"\"\"Constructor.\"\"\"\n",
    "\n",
    "        super().__init__()\n",
    "\n",
    "        self.sublayer = nn.ModuleList([sublayer, nn.Dropout(dropout_rate)])\n",
    "        self.layer_norm = nn.LayerNorm(d_embed)\n",
    "\n",
    "    def forward(self, x, y: torch.tensor = None, z: torch.tensor = None):\n",
    "        \"\"\"Forward Pass.\"\"\"\n",
    "\n",
    "        out = x.clone()\n",
    "\n",
    "        for layer in self.sublayer:\n",
    "            if type(layer) == MultiHeadAttention and layer.mask:\n",
    "                out = layer(out, y, z)\n",
    "            else:\n",
    "                out = layer(out)\n",
    "        x = x + out\n",
    "\n",
    "        return self.layer_norm(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7dd84fac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output shape : torch.Size([3, 2, 64])\n",
      "Output shape : torch.Size([3, 2, 64])\n"
     ]
    }
   ],
   "source": [
    "## testing ##\n",
    "\n",
    "## For reproducibility ##\n",
    "torch.manual_seed(97)\n",
    "\n",
    "x = torch.randn(3 , 2 , 64)\n",
    "\n",
    "rdn = ResidualDropoutNorm(MultiHeadAttention(d_embed=64, num_heads=8), 64)\n",
    "\n",
    "print(f'Output shape : {rdn(x).shape}')\n",
    "\n",
    "rdn = ResidualDropoutNorm(PositionWiseFFN(d_embed=64), 64)\n",
    "\n",
    "print(f'Output shape : {rdn(x).shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dac8da8",
   "metadata": {},
   "source": [
    "And it matches...!! \n",
    "\n",
    "I am really happy to be honest, and if you are reading this and trying to build it out yourself and you are at this point now, you should be very proud of yourself, because we are almost done and just need to plug in all these modular pieces together to make our final architecture."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70e43699",
   "metadata": {},
   "source": [
    "Now its time to make the layers of the Encoder and the Decoder, which are repeated N times to make the encoder and decoder of the Transformer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "31a708aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Encoder Layer ##\n",
    "\n",
    "class EncoderLayer(nn.Module):\n",
    "    \"\"\"Encoder Layer.\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        d_embed: int,\n",
    "        num_heads: int,\n",
    "        dropout_rate: float,\n",
    "        d_k: int = None,\n",
    "        mask: bool = False,\n",
    "    ):\n",
    "        \"\"\"Constructor.\"\"\"\n",
    "\n",
    "        super().__init__()\n",
    "\n",
    "        self.encoder_layer = nn.ModuleList(\n",
    "            [\n",
    "                ResidualDropoutNorm(\n",
    "                    d_embed=d_embed,\n",
    "                    sublayer=MultiHeadAttention(\n",
    "                        d_embed=d_embed,\n",
    "                        num_heads=num_heads,\n",
    "                        d_k=d_k,\n",
    "                        mask=mask,\n",
    "                    ),\n",
    "                    dropout_rate=dropout_rate,\n",
    "                ),\n",
    "                ResidualDropoutNorm(\n",
    "                    d_embed=d_embed,\n",
    "                    sublayer=PositionWiseFFN(d_embed),\n",
    "                    dropout_rate=dropout_rate,\n",
    "                ),\n",
    "            ]\n",
    "        )\n",
    "\n",
    "    def forward(self, x, y: torch.tensor = None, z: torch.tensor = None):\n",
    "        \"\"\"Forward pass through a single encoder layer\"\"\"\n",
    "\n",
    "        for layer in self.encoder_layer:\n",
    "            out = layer(x, y, z)\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "91d59ca9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output shape : torch.Size([4, 8, 512])\n"
     ]
    }
   ],
   "source": [
    "## Testing ##\n",
    "\n",
    "## For reproducibility ##\n",
    "torch.manual_seed(97)\n",
    "\n",
    "enc_layer = EncoderLayer(d_embed = 512, num_heads = 8, dropout_rate = 0.1)\n",
    "\n",
    "x = torch.randn(4, 8, 512)\n",
    "\n",
    "print(f'Output shape : {enc_layer(x).shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87986894",
   "metadata": {},
   "source": [
    "Perfect... Looks awesome...\n",
    "\n",
    "Next up is the decoder layer!! Here we must be a bit careful since we need to make two attention heads. One masked attention and another without masking. Also we cannot put everything in `nn.Sequential` since the second attention head receives different inputs. So, lets slowly and carefully build it up."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2b14c66c",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Decoder Layer ##\n",
    "\n",
    "class DecoderLayer(nn.Module):\n",
    "    \"\"\"Implements a single decoder layer.\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        d_embed: int,\n",
    "        num_heads: int,\n",
    "        dropout_rate: float,\n",
    "        d_k_sa: int = None,\n",
    "        use_ca: bool = False,\n",
    "        d_k_ca: int = None,\n",
    "    ):\n",
    "\n",
    "        \"\"\"Constructor.\"\"\"\n",
    "\n",
    "        super().__init__()\n",
    "\n",
    "        self.masked_attn = ResidualDropoutNorm(\n",
    "            sublayer=MultiHeadAttention(\n",
    "                d_embed=d_embed,\n",
    "                num_heads=num_heads,\n",
    "                d_k=d_k_sa,\n",
    "                mask=True,\n",
    "            ),\n",
    "            d_embed=d_embed,\n",
    "            dropout_rate=dropout_rate,\n",
    "        )\n",
    "\n",
    "        self.cross_attn = (\n",
    "            ResidualDropoutNorm(\n",
    "                sublayer=MultiHeadAttention(\n",
    "                    d_embed=d_embed,\n",
    "                    num_heads=num_heads,\n",
    "                    d_k=d_k_ca,\n",
    "                    mask=False,\n",
    "                ),\n",
    "                d_embed=d_embed,\n",
    "                dropout_rate=dropout_rate,\n",
    "            )\n",
    "            if use_ca\n",
    "            else None\n",
    "        )\n",
    "\n",
    "        self.pwfn = ResidualDropoutNorm(\n",
    "            d_embed=d_embed,\n",
    "            sublayer=PositionWiseFFN(d_embed),\n",
    "            dropout_rate=dropout_rate,\n",
    "        )\n",
    "\n",
    "    def forward(self, z: torch.tensor, x: torch.tensor = None, y: torch.tensor = None):\n",
    "        \"\"\"Forward Pass through a single decoder layer.\"\"\"\n",
    "\n",
    "        z = self.masked_attn(x=z)\n",
    "\n",
    "        if self.cross_attn != None:\n",
    "            z = self.cross_attn(x, y, z)\n",
    "\n",
    "        return self.pwfn(z)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3761c9a7",
   "metadata": {},
   "source": [
    "Lets test it out!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "36678c77",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output shape : torch.Size([3, 8, 64])\n"
     ]
    }
   ],
   "source": [
    "## Testing ##\n",
    "\n",
    "## For reproducing ##\n",
    "torch.manual_seed(97)\n",
    "\n",
    "decoder_layer = DecoderLayer(d_embed = 64, num_heads = 8, dropout_rate = 0.1, use_ca = True, d_k_ca = 128)\n",
    "\n",
    "from_encoder = torch.randn(3 , 8 , 64)\n",
    "from_decoder = torch.randn(3 , 8 , 64)\n",
    "\n",
    "print(f'Output shape : {decoder_layer(z = from_decoder, x= from_encoder).shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17baf8b8",
   "metadata": {},
   "source": [
    "Wow!! That's a lot of good work... Now let's move on and finally make our encoder and decoder blocks and finally the full Transformer block."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "90d1da59",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Encoder block ##\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "    \"\"\"Encoder Block\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        d_embed: int,\n",
    "        num_heads: int,\n",
    "        dropout_rate: float,\n",
    "        d_k: int = None,\n",
    "        mask: bool = False,\n",
    "        num_layers: int = 6,\n",
    "    ):\n",
    "        \"\"\"Constructor\"\"\"\n",
    "\n",
    "        super().__init__()\n",
    "        self.encoder = nn.ModuleList(\n",
    "            [\n",
    "                EncoderLayer(d_embed, num_heads, dropout_rate, d_k, mask)\n",
    "                for _ in range(num_layers)\n",
    "            ]\n",
    "        )\n",
    "\n",
    "    def forward(self, inp, return_intermediate=False):\n",
    "        \"\"\"Forward Pass\"\"\"\n",
    "\n",
    "        intermediate_val = torch.tensor([])\n",
    "\n",
    "        for layer in self.encoder:\n",
    "            inp = layer(inp)\n",
    "            intermediate_val = torch.cat([intermediate_val, inp.unsqueeze(0)], dim=0)\n",
    "\n",
    "        if return_intermediate:\n",
    "            return intermediate_val[-1], intermediate_val\n",
    "\n",
    "        return intermediate_val[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "14e797a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output is torch.Size([2, 2, 64])\n",
      "Intermediate shape is torch.Size([6, 2, 2, 64])\n",
      "Intermediate values are : tensor([[[[ 1.8488, -0.6840, -0.2318,  ..., -1.0235,  0.4784, -0.7230],\n",
      "          [ 0.2811, -0.7021, -0.2585,  ..., -0.8179, -1.2603, -1.2209]],\n",
      "\n",
      "         [[-0.6921,  1.5826,  0.3856,  ...,  0.2469,  0.6324,  0.5615],\n",
      "          [-0.0306,  0.0901,  1.0393,  ..., -0.6681,  1.4845, -1.3618]]],\n",
      "\n",
      "\n",
      "        [[[ 2.3459, -1.0848,  0.2010,  ..., -1.1772,  0.5774, -0.8000],\n",
      "          [ 0.4959, -0.6215, -0.2967,  ..., -0.5862, -1.1931, -1.1484]],\n",
      "\n",
      "         [[-0.6813,  1.3622,  0.4601,  ...,  0.3345,  0.7836,  0.3445],\n",
      "          [ 0.1517,  0.0620,  0.7610,  ..., -0.4687,  1.2950, -1.3701]]],\n",
      "\n",
      "\n",
      "        [[[ 2.3232, -1.3206,  0.1268,  ..., -1.2306,  0.9281, -1.0693],\n",
      "          [ 0.5146, -0.6943, -0.1544,  ..., -0.9186, -0.7569, -1.0942]],\n",
      "\n",
      "         [[-0.0967,  1.2268,  0.3612,  ..., -0.0602,  1.2503,  0.2629],\n",
      "          [ 0.0069, -0.0968,  0.9283,  ..., -0.6343,  1.7400, -1.0824]]],\n",
      "\n",
      "\n",
      "        [[[ 2.0574, -1.1008, -0.2540,  ..., -1.8702,  0.9604, -1.2660],\n",
      "          [ 0.5137, -0.4902, -0.3883,  ..., -1.3065, -0.8214, -1.0865]],\n",
      "\n",
      "         [[-0.1591,  0.9358,  0.1123,  ..., -0.3495,  0.6052,  0.5156],\n",
      "          [ 0.0432, -0.1926,  1.0328,  ..., -1.1056,  1.7212, -1.0604]]],\n",
      "\n",
      "\n",
      "        [[[ 2.0388, -0.7986, -0.1725,  ..., -2.0374,  0.8239, -1.1827],\n",
      "          [ 0.7479, -0.1174, -0.3237,  ..., -1.6938, -0.8154, -0.7228]],\n",
      "\n",
      "         [[-0.0493,  1.2723,  0.0361,  ..., -0.6347,  0.8719,  0.9613],\n",
      "          [-0.0739, -0.3176,  0.7881,  ..., -1.3696,  1.8618, -0.8786]]],\n",
      "\n",
      "\n",
      "        [[[ 1.8045, -0.5798, -0.1419,  ..., -1.6324,  0.8031, -1.1913],\n",
      "          [ 0.7009,  0.3262, -0.1766,  ..., -1.6586, -0.8488, -0.7425]],\n",
      "\n",
      "         [[ 0.1063,  1.8336,  0.4276,  ..., -0.4066,  0.6317,  0.8273],\n",
      "          [-0.5937,  0.1511,  0.7232,  ..., -1.3680,  1.8536, -0.7647]]]],\n",
      "       grad_fn=<CatBackward0>)\n"
     ]
    }
   ],
   "source": [
    "## Testing ##\n",
    "\n",
    "torch.manual_seed(97)\n",
    "\n",
    "enc = Encoder(d_embed=64, num_heads=4, dropout_rate=0.1)\n",
    "\n",
    "inp = torch.randn(2,2,64)\n",
    "\n",
    "print(f'Output is {enc(inp).shape}')\n",
    "\n",
    "print(f'Intermediate shape is {enc(inp,True)[-1].shape}')\n",
    "\n",
    "intermediate_values = enc(inp,True)[-1]\n",
    "print(f'Intermediate values are : {intermediate_values}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b684926",
   "metadata": {},
   "source": [
    "Perfect... Looks nice!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89eab605",
   "metadata": {},
   "source": [
    "Now we will build the Decoder Block."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "9672eb8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Decoder Block ##\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    \"\"\"Decoder Block\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        d_embed: int,\n",
    "        num_heads: int,\n",
    "        dropout_rate: float,\n",
    "        d_k_sa: int = None,\n",
    "        use_ca: bool = False,\n",
    "        d_k_ca: int = None,\n",
    "        num_layers: int = 6,\n",
    "    ):\n",
    "\n",
    "        super().__init__()\n",
    "\n",
    "        self.decoder = nn.ModuleList(\n",
    "            [\n",
    "                DecoderLayer(\n",
    "                    d_embed,\n",
    "                    num_heads,\n",
    "                    dropout_rate,\n",
    "                    d_k_sa,\n",
    "                    use_ca,\n",
    "                    d_k_ca,\n",
    "                )\n",
    "                for _ in range(num_layers)\n",
    "            ]\n",
    "        )\n",
    "\n",
    "    def forward(self, z, intermediate_value=None):\n",
    "        \"\"\"Forward Pass\"\"\"\n",
    "\n",
    "        for i, layer in enumerate(self.decoder):\n",
    "            if intermediate_value != None:\n",
    "                z = layer(z=z, x=intermediate_value[i])\n",
    "\n",
    "            else:\n",
    "                z = layer(z=z)\n",
    "\n",
    "        return z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "2c750e0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 2, 64])\n",
      "Output shape : torch.Size([2, 2, 64])\n"
     ]
    }
   ],
   "source": [
    "## testing ##\n",
    "\n",
    "torch.manual_seed(97)\n",
    "\n",
    "inp = torch.randn(2,2,64)\n",
    "\n",
    "encoder = Encoder(d_embed=64, num_heads=4, dropout_rate=0.1)\n",
    "\n",
    "_ , intermediate_val = encoder(inp, return_intermediate = True)\n",
    "\n",
    "decoder = Decoder(d_embed=64, num_heads=4, dropout_rate=0.1, use_ca = True)\n",
    "\n",
    "print(inp.shape)\n",
    "\n",
    "print(f'Output shape : {decoder(inp, intermediate_val).shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fcf8374",
   "metadata": {},
   "source": [
    "Amazing we are done with almost everything. Now what we just need to do is plug in everything in our **Transformers** module."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "dc025c28",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer(nn.Module):\n",
    "    \"\"\"Transformer module.\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        vocab_size: int,\n",
    "        sequence_length: int,\n",
    "        d_embed: int,\n",
    "        use_encoder: bool,\n",
    "        use_decoder: bool,\n",
    "        num_heads: int,\n",
    "        d_k_encoder: int = None,\n",
    "        encoder_mask: bool = False,\n",
    "        encoder_num_layers: int = 6,\n",
    "        decoder_num_layers: int = 6,\n",
    "        dropout_rate: float = 0.1,\n",
    "        classification: bool = False,\n",
    "        num_classes: int = None,\n",
    "        use_ca: bool = False,\n",
    "        d_k_sa: int = None,\n",
    "        d_k_ca: int = None,\n",
    "        mask: bool = False,\n",
    "        device: torch.device = torch.device(\"cpu\"),\n",
    "    ):\n",
    "        \"\"\"Constructor\"\"\"\n",
    "\n",
    "        super().__init__()\n",
    "\n",
    "        self.embed = nn.Embedding(vocab_size, d_embed)\n",
    "\n",
    "        # self.pos_embed = PositionalEncoding(sequence_length, d_embed, device)\n",
    "\n",
    "        self.final_output = (\n",
    "            nn.Linear(d_embed, num_classes)\n",
    "            if classification\n",
    "            else nn.Linear(d_embed, vocab_size)\n",
    "        )\n",
    "\n",
    "        self.encoder = None\n",
    "        self.decoder = None\n",
    "\n",
    "        if use_encoder:\n",
    "            self.encoder = Encoder(\n",
    "                d_embed,\n",
    "                num_heads,\n",
    "                dropout_rate,\n",
    "                d_k_encoder,\n",
    "                mask,\n",
    "                num_layers=encoder_num_layers,\n",
    "            )\n",
    "\n",
    "        if use_decoder:\n",
    "            self.decoder = Decoder(\n",
    "                d_embed,\n",
    "                num_heads,\n",
    "                dropout_rate,\n",
    "                d_k_sa,\n",
    "                use_ca,\n",
    "                d_k_ca,\n",
    "                num_layers=decoder_num_layers,\n",
    "            )\n",
    "\n",
    "        self.device = device\n",
    "        self.register_buffer(\"use_ca\", torch.tensor([use_ca]))\n",
    "        self.register_buffer(\"decoder_num_layers\", torch.tensor([decoder_num_layers]))\n",
    "        self.register_buffer(\"sequence_length\", torch.tensor([sequence_length]))\n",
    "        self.register_buffer(\"classification\", torch.tensor([classification]))\n",
    "\n",
    "    def forward(self, inputs=None, outputs=None):\n",
    "        \"\"\"Forward Pass.\"\"\"\n",
    "\n",
    "        intermediate = None\n",
    "\n",
    "        if self.encoder != None:\n",
    "            inputs = self.embed(inputs)\n",
    "            B, L, C = inputs.shape\n",
    "            pos_embed = PositionalEncoding(L, C, self.device)\n",
    "            inputs = inputs + pos_embed\n",
    "            inputs, intermediate = self.encoder(inputs, return_intermediate=True)\n",
    "\n",
    "            if self.decoder == None or self.classification.item() == True:\n",
    "                return self.final_output(inputs)\n",
    "\n",
    "        if self.decoder != None:\n",
    "            outputs = self.embed(outputs)\n",
    "            B, L, C = outputs.shape\n",
    "            pos_embed = PositionalEncoding(L, C, self.device)\n",
    "            outputs = outputs + pos_embed\n",
    "\n",
    "            if (intermediate != None and self.use_ca.item() == True) or (\n",
    "                intermediate != None\n",
    "            ):\n",
    "                outputs = self.decoder(outputs, intermediate)\n",
    "\n",
    "            elif self.use_ca:\n",
    "                intermediate = torch.cat(\n",
    "                    [outputs.unsqueeze(0) for _ in range(self.decoder_num_layers)],\n",
    "                    dim=0,\n",
    "                )\n",
    "                outputs = self.decoder(outputs, intermediate)\n",
    "\n",
    "            else:\n",
    "                outputs = self.decoder(outputs)\n",
    "\n",
    "        return self.final_output(outputs)\n",
    "\n",
    "    ## Utility function to show data ##\n",
    "\n",
    "    def _show_data(self, data, idx_2_char_map, verbose=True):\n",
    "        \"\"\"Given a data tensor, maps them to string and prints them.\"\"\"\n",
    "        str_data = [idx_2_char_map[each_word.item()] for each_word in data.data]\n",
    "        if verbose:\n",
    "            print(str_data)\n",
    "        else:\n",
    "            return str_data\n",
    "\n",
    "    def generate(\n",
    "        self,\n",
    "        max_length: int,\n",
    "        idx_2_char_map: dict,\n",
    "        device: torch.device = torch.device(\"cpu\"),\n",
    "    ):\n",
    "        \"\"\"Generative bit.\"\"\"\n",
    "\n",
    "        idx = torch.zeros((1, 1), device=device).int()\n",
    "\n",
    "        assert self.decoder != None, \"Decoder must be present!!\"\n",
    "\n",
    "        for _ in range(max_length):\n",
    "            logits = self(outputs=idx[:, -self.sequence_length :])\n",
    "            logits = logits[:, -1, :]\n",
    "            prob = torch.nn.functional.softmax(logits, dim=-1)\n",
    "            next_idx = torch.multinomial(prob, 1).int()\n",
    "            idx = torch.cat([idx, next_idx], dim=1)\n",
    "\n",
    "        print(\"\".join(self._show_data(idx[0, 1:].data, idx_2_char_map, verbose=False)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "01546850",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------\n",
      "Testing just decoder without encoder\n",
      "------------------------------------\n",
      "Outputs shape : torch.Size([2, 8])\n",
      "Outputs shape : torch.Size([2, 8, 6000])\n",
      "------------------------------------\n",
      "Testing just decoder without encoder \n",
      "but with cross attention in between\n",
      "------------------------------------\n",
      "Outputs shape : torch.Size([2, 8])\n",
      "Outputs shape : torch.Size([2, 8, 6000])\n",
      "------------------------------------\n",
      "Testing encoder and decoder\n",
      "------------------------------------\n",
      "Inputs shape : torch.Size([2, 8])\n",
      "Outputs shape : torch.Size([2, 8])\n",
      "Outputs shape : torch.Size([2, 8, 6000])\n"
     ]
    }
   ],
   "source": [
    "## Final test for Transformer module ##\n",
    "\n",
    "torch.manual_seed(97)\n",
    "\n",
    "print(f'------------------------------------')\n",
    "print(f'Testing just decoder without encoder')\n",
    "print(f'------------------------------------')\n",
    "\n",
    "transformer_1 = Transformer(vocab_size = 6000,\n",
    "                          sequence_length = 8,\n",
    "                          d_embed = 32,\n",
    "                          use_encoder=False,\n",
    "                          use_decoder=True,\n",
    "                          num_heads=8,\n",
    "                          )\n",
    "\n",
    "\n",
    "outputs_1 = torch.randint(0, 6000, (2, 8))\n",
    "\n",
    "print(f'Outputs shape : {outputs_1.shape}')\n",
    "\n",
    "print(f'Outputs shape : {transformer_1(outputs=outputs_1).shape}')\n",
    "\n",
    "print(f'------------------------------------')\n",
    "print(f'Testing just decoder without encoder \\nbut with cross attention in between')\n",
    "print(f'------------------------------------')\n",
    "\n",
    "transformer_2 = Transformer(vocab_size = 6000,\n",
    "                          sequence_length = 8,\n",
    "                          d_embed = 32,\n",
    "                          use_encoder=False,\n",
    "                          use_decoder=True,\n",
    "                          num_heads=8,\n",
    "                          use_ca=True\n",
    "                          )\n",
    "\n",
    "outputs_2 = torch.randint(0, 6000, (2, 8))\n",
    "\n",
    "print(f'Outputs shape : {outputs_2.shape}')\n",
    "\n",
    "print(f'Outputs shape : {transformer_2(outputs=outputs_2).shape}')\n",
    "\n",
    "print(f'------------------------------------')\n",
    "print(f'Testing encoder and decoder')\n",
    "print(f'------------------------------------')\n",
    "\n",
    "inputs = torch.randint(0, 6000, (2, 8))\n",
    "outputs = torch.randint(0, 6000, (2, 8))\n",
    "\n",
    "transformer_2 = Transformer(vocab_size = 6000,\n",
    "                          sequence_length = 8,\n",
    "                          d_embed = 32,\n",
    "                          use_encoder=True,\n",
    "                          use_decoder=True,\n",
    "                          num_heads=8,\n",
    "                          use_ca=True\n",
    "                          )\n",
    "\n",
    "print(f'Inputs shape : {inputs.shape}')\n",
    "print(f'Outputs shape : {outputs.shape}')\n",
    "\n",
    "print(f'Outputs shape : {transformer_2(inputs=inputs,outputs=outputs).shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "475f7e75",
   "metadata": {},
   "source": [
    "Perfect... We have coded up everything that we need to build our transformer model. Now what I will do is put all these codes in a python module so that in the next Notebook we can test our Transformer block with a real world data.... See ya!!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch_env",
   "language": "python",
   "name": "pytorch_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16 (default, Mar  2 2023, 03:18:16) [MSC v.1916 64 bit (AMD64)]"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
