{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"## Implementing Style Transfer using AdaIN","metadata":{}},{"cell_type":"markdown","source":"In this notebook we are going to implement Adaptive Instance Normalization based on the paper \"Arbitrary Style Transfer in Real-time with Adaptive Instance Normalization\" by Huang et. al. All the implementation will be done via pytorch. So, let dive into it.","metadata":{}},{"cell_type":"markdown","source":"We are going to first import all the necessary packages.","metadata":{}},{"cell_type":"code","source":"## Importing necessary packages ##\n\nimport torch\nimport torchvision\nimport torch.nn as nn\nfrom torch.utils.data import Dataset, DataLoader\nfrom torchvision.utils import make_grid\nfrom torchvision.models import vgg19\nfrom torchvision.transforms import transforms\n\nfrom tqdm import tqdm\nimport PIL\nimport os\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns","metadata":{"execution":{"iopub.status.busy":"2021-07-22T07:39:51.945586Z","iopub.execute_input":"2021-07-22T07:39:51.946313Z","iopub.status.idle":"2021-07-22T07:39:54.757092Z","shell.execute_reply.started":"2021-07-22T07:39:51.946146Z","shell.execute_reply":"2021-07-22T07:39:54.756003Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Now, first up is importing the datasets.\n\nSince this is a style transfer model, we will have a content image and a style image. Hence, we are going to load two datasets. So, lets go and do that in the next step.","metadata":{}},{"cell_type":"code","source":"## Making our custom dataset ##\n\nclass CustomDataset(Dataset):\n    \n    def __init__(self , content_dir , style_dir):\n        \n        super().__init__()\n        \n        self.content_dir = content_dir\n        \n        self.content_imgs = os.listdir(content_dir)\n        \n        self.style_dir = style_dir\n        \n        self.style_imgs = os.listdir(style_dir)\n        \n    def __getitem__(self , idx):\n        \n        content_idx = idx % len(self.content_imgs)\n        \n        style_idx = idx % len(self.style_imgs)\n        \n        content_arr =  PIL.Image.open(os.path.join(self.content_dir , self.content_imgs[content_idx]))\n        \n        style_arr =  PIL.Image.open(os.path.join(self.style_dir , self.style_imgs[style_idx]))\n        \n        aug = transforms.Compose([\n            transforms.Resize((256 , 256)),\n            transforms.ToTensor()\n        ])\n        \n        content_tensor = aug(content_arr)\n        \n        style_tensor = aug(style_arr)\n        \n        return content_tensor , style_tensor\n    \n    def __len__(self):\n        \n        content_length = len(self.content_imgs)\n        \n        return content_length \n\n## Defining our dataset instance ##\n\ndataset = CustomDataset('../input/style-content-data/mscoco_resized/train2014','../input/style-content-data/Abstract_gallery')","metadata":{"execution":{"iopub.status.busy":"2021-07-22T07:39:54.759084Z","iopub.execute_input":"2021-07-22T07:39:54.75953Z","iopub.status.idle":"2021-07-22T07:39:58.468019Z","shell.execute_reply.started":"2021-07-22T07:39:54.759485Z","shell.execute_reply":"2021-07-22T07:39:58.466803Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"As always it's better to visualize a datapoint. We are going to do that next.","metadata":{}},{"cell_type":"code","source":"random_idx = int(np.random.randint(low = 0 , high = len(dataset) , size = 1))\n\ncontent , style = dataset[random_idx]\n\nprint('Content Image --> Maximum value : {} , Minimum value : {}'.format(torch.max(content) , torch.min(content)))\n\nprint('Style Image --> Maximum value : {} , Minimum value : {}'.format(torch.max(style) , torch.min(style)))\n\nplt.imshow(content.permute(1 , 2 , 0))\n\nplt.show()\n\nplt.imshow(style.permute(1 , 2 , 0))\n\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-07-22T07:39:58.472589Z","iopub.execute_input":"2021-07-22T07:39:58.472952Z","iopub.status.idle":"2021-07-22T07:39:58.95962Z","shell.execute_reply.started":"2021-07-22T07:39:58.472922Z","shell.execute_reply":"2021-07-22T07:39:58.958443Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Our dataset is fine.\n\nLets now go on and make a dataloader which would feed in data to the model in batches. Each batch will contain 16 data points. Also we are going to visualize the data by using an utility function.","metadata":{}},{"cell_type":"code","source":"## Making our dataloader ##\n\nst_dataloader = DataLoader(dataset , batch_size = 32 , shuffle = True)\n\n\n## Visualization utility function ##\n\ndef visualize(imgs):\n    \n    fig , ax = plt.subplots(figsize = (8 , 4))\n    \n    ax.set_xticklabels([])\n    \n    ax.set_yticklabels([])\n    \n    plt.imshow(make_grid(imgs.detach().to('cpu') , 4).permute(1 , 2 , 0))\n    \n    plt.show()\n    \n\n## Visualizing a batch of data ##\n\nfor content , style in st_dataloader:\n    \n    visualize(content)\n    \n    visualize(style)\n    \n    break","metadata":{"execution":{"iopub.status.busy":"2021-07-22T07:39:58.962131Z","iopub.execute_input":"2021-07-22T07:39:58.962603Z","iopub.status.idle":"2021-07-22T07:40:01.108186Z","shell.execute_reply.started":"2021-07-22T07:39:58.962558Z","shell.execute_reply":"2021-07-22T07:40:01.106902Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"With that we have set up our dataset as well as dataloader.\n\nBut the thing is we have utilized the GPU of our machine. All our dataloader batches will be in cpu. To transfer the data to GPU we need to do some more work. Lets do that now!","metadata":{}},{"cell_type":"code","source":"## Setting the device to cpu or gpu ##\n\ndef set_device():\n    \n    if torch.cuda.is_available():\n        \n        return torch.device('cuda')\n    \n    return torch.device('cpu')\n\ndevice = set_device()\n\n\n## Utility function to transfer data to the specified device ##\n\ndef transfer_data(data , device):\n    \n    if isinstance(data , (tuple , list)):\n        \n        return [transfer_data(each_data , device) for each_data in data]\n    \n    return data.to(device)\n\n\n## GPU dataloader class ##\n\nclass GPUDataloader:\n    \n    def __init__(self , dl , device):\n        \n        self.dl = dl\n        \n        self.device = device\n        \n    def __iter__(self):\n        \n        for batch in self.dl:\n            \n            yield transfer_data(batch , self.device)\n            \n    def __len__(self):\n        \n        return len(self.dl)","metadata":{"execution":{"iopub.status.busy":"2021-07-22T07:40:01.110015Z","iopub.execute_input":"2021-07-22T07:40:01.110518Z","iopub.status.idle":"2021-07-22T07:40:01.186608Z","shell.execute_reply.started":"2021-07-22T07:40:01.110471Z","shell.execute_reply":"2021-07-22T07:40:01.185341Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Now with all that out of the way, lets create an instance of the dataloader and as always visualize the data.","metadata":{}},{"cell_type":"code","source":"## Creating dataloader instance ##\n\nstyletransfer_dl = GPUDataloader(st_dataloader , device)\n\n\n## Visualizing a batch of dataloader ##\n\nfor content , style in styletransfer_dl: \n\n    visualize(content)\n    \n    visualize(style)\n    \n    break","metadata":{"execution":{"iopub.status.busy":"2021-07-22T07:40:01.188479Z","iopub.execute_input":"2021-07-22T07:40:01.189314Z","iopub.status.idle":"2021-07-22T07:40:08.932047Z","shell.execute_reply.started":"2021-07-22T07:40:01.189266Z","shell.execute_reply":"2021-07-22T07:40:08.930958Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"With the initial things done, lets get our hands dirty and model our neural network.\n\nFirst off, lets set our encoder.","metadata":{}},{"cell_type":"code","source":"## Setting our encoder part ##\n\nclass Encoder(nn.Module):\n    \n    def __init__(self):\n        \n        super().__init__()\n        \n        self.model = vgg19(pretrained = True , progress= True)\n                \n        self.features = self.model.features[:21]\n        \n        for param in self.features.parameters():\n            \n            param.requires_grad = False\n        \n    def forward(self , x):\n        \n        out = self.features(x)\n        \n        return out\n    \n## Testing our encoder ##\n\ntest_img = torch.randn(1 , 3 , 256 , 256)\n\ntest_encoder = Encoder()\n\ntest_out = test_encoder(test_img)\n\nprint('The output shape is :' , test_out.shape)","metadata":{"execution":{"iopub.status.busy":"2021-07-22T07:40:08.933639Z","iopub.execute_input":"2021-07-22T07:40:08.934094Z","iopub.status.idle":"2021-07-22T07:40:17.086406Z","shell.execute_reply.started":"2021-07-22T07:40:08.93405Z","shell.execute_reply":"2021-07-22T07:40:17.085156Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Our Encoder is working perfectly then!","metadata":{}},{"cell_type":"markdown","source":"Next up lets build our Adaptive Instance Normalization module.","metadata":{}},{"cell_type":"code","source":"## Creating AdaIN ##\n\nclass AdaIN(nn.Module):\n    \n    def _init__(self):\n        \n        super().__init__()\n        \n    def forward(self , content , style):\n        \n        content_mean = torch.mean(content , dim = [2 , 3] , keepdims = False).unsqueeze(2).unsqueeze(3).expand_as(content)\n        \n        content_std = torch.std(content , dim = [2 , 3] , keepdims = False).unsqueeze(2).unsqueeze(3).expand_as(content)\n        \n        style_mean = torch.mean(style , dim = [2 , 3] , keepdims = False).unsqueeze(2).unsqueeze(3).expand_as(content)\n        \n        style_std = torch.std(style , dim = [2 , 3] , keepdims = False).unsqueeze(2).unsqueeze(3).expand_as(content)\n        \n        out = style_std * ((content - content_mean) / (content_std + 1e-5)) + style_mean\n        \n        return out , style_mean , style_std\n    \n    \n## Testing our AdaIN ##\n\ntest_content = torch.randn(1 , 512 , 32 , 32)\n\ntest_style = torch.randn(1 , 512 , 32 , 32)\n\ntest_adain = AdaIN()\n\ntest_out , _ , _ = test_adain(test_content , test_style)\n\nprint('Output Shape :' , test_out.shape)","metadata":{"execution":{"iopub.status.busy":"2021-07-22T07:40:17.089676Z","iopub.execute_input":"2021-07-22T07:40:17.090154Z","iopub.status.idle":"2021-07-22T07:40:17.138742Z","shell.execute_reply.started":"2021-07-22T07:40:17.090106Z","shell.execute_reply":"2021-07-22T07:40:17.137301Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Now we are going to make our VGG19 decoder.\n\n-- Replace the Pooling with upsampling.","metadata":{}},{"cell_type":"code","source":"## Creating the VGG19 decoder module ##\n\nclass Decoder(nn.Module):\n    \n    def __init__(self , in_dim = 512 , out_dim = 3 , kernel = 3, k_stride = 1 , \n                 padding = 1 , scale = 2):\n        \n        super().__init__()\n        \n        self.net = nn.Sequential(nn.Conv2d(in_channels = in_dim , out_channels = in_dim // 2 , kernel_size = kernel,\n                                           stride = k_stride , padding = padding),\n                                 nn.Upsample(scale_factor = scale),\n                                 nn.ReLU(),\n                                 nn.Conv2d(in_channels = in_dim // 2 , out_channels = in_dim // 2 , kernel_size = kernel,\n                                           stride = k_stride , padding = padding),\n                                 nn.ReLU(),\n                                 nn.Conv2d(in_channels = in_dim // 2 , out_channels = in_dim // 2 , kernel_size = kernel,\n                                           stride = k_stride , padding = padding),\n                                 nn.ReLU(),\n                                 nn.Conv2d(in_channels = in_dim // 2 , out_channels = in_dim // 2 , kernel_size = kernel,\n                                           stride = k_stride , padding = padding),\n                                 nn.ReLU(),\n                                 nn.Conv2d(in_channels = in_dim // 2 , out_channels = in_dim // 4 , kernel_size = kernel,\n                                           stride = k_stride , padding = padding),\n                                 nn.Upsample(scale_factor = scale),\n                                 nn.ReLU(),\n                                 nn.Conv2d(in_channels = in_dim // 4 , out_channels = in_dim // 4 , kernel_size = kernel,\n                                           stride = k_stride , padding = padding),\n                                 nn.ReLU(),\n                                 nn.Conv2d(in_channels = in_dim // 4 , out_channels = in_dim // 8 , kernel_size = kernel,\n                                           stride = k_stride , padding = padding),\n                                 nn.Upsample(scale_factor = scale),\n                                 nn.ReLU(),\n                                 nn.Conv2d(in_channels = in_dim // 8 , out_channels = in_dim // 8 , kernel_size = kernel,\n                                           stride = k_stride , padding = padding),\n                                 nn.ReLU(),\n                                 nn.Conv2d(in_channels = in_dim // 8 , out_channels = out_dim , kernel_size = kernel,\n                                           stride = k_stride , padding = padding)\n                                )\n        \n    def forward(self , x):\n        \n        out = self.net(x)\n        \n        return out\n    \n\n## Testing our decoder ##\n\ntest_inp = torch.randn(1 , 512 , 32 , 32)\n\ntest_decoder = Decoder()\n\ntest_out = test_decoder(test_inp)\n\nprint('Shape of output :' , test_out.shape)","metadata":{"execution":{"iopub.status.busy":"2021-07-22T07:40:17.140846Z","iopub.execute_input":"2021-07-22T07:40:17.141278Z","iopub.status.idle":"2021-07-22T07:40:17.822344Z","shell.execute_reply.started":"2021-07-22T07:40:17.141235Z","shell.execute_reply":"2021-07-22T07:40:17.821154Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"All right.\n\nOur modules are done.\n\nNow lets put all of them together and make our model.","metadata":{}},{"cell_type":"code","source":"## Joining the pieces and making our final model ##\n\nclass StyleTransfer(nn.Module):\n    \n    def __init__(self):\n        \n        super().__init__()\n        \n        self.encoder = Encoder()\n        \n        self.decoder = Decoder()\n        \n        self.adain = AdaIN()\n        \n    def forward(self , content , style):\n        \n        content_feature = self.encoder(content)\n        \n        style_feature = self.encoder(style)\n        \n        stylized_feature , style_mean , style_std = self.adain(content_feature , style_feature)\n        \n        stylized_img = self.decoder(stylized_feature)\n        \n        content_final_map = self.encoder(stylized_img)\n        \n        final_mean = torch.mean(content_final_map , dim = [2 , 3] , \n                                keepdims = False).unsqueeze(2).unsqueeze(3).expand_as(style_mean)\n        \n        final_std = torch.std(content_final_map , dim = [2 , 3] , \n                              keepdims = False).unsqueeze(2).unsqueeze(3).expand_as(style_std)\n        \n        return stylized_img , content_final_map , stylized_feature , style_mean , style_std , final_mean , final_std\n    \n## Testing our model ##\n\ntest_content = torch.randn(1 , 3 , 256 , 256)\n\ntest_style = torch.randn(1 , 3 , 256 , 256)\n\ntest_model = StyleTransfer()\n\nstylized_img , content_final_map , stylized_feature , style_mean , style_std , final_mean , final_std = test_model(test_content\n                                                                                                                   , test_style)\n\nprint('Stylized Image shape :' , stylized_img.shape)\n\nprint('Content final map shape :' , content_final_map.shape)\n\nprint('Stylized map shape :' , stylized_feature.shape)\n\nprint('Style mean shape :' , style_mean.shape)\n\nprint('Style std shape :' , style_std.shape)\n\nprint('Final mean shape :' , final_mean.shape)\n\nprint('Final std shape :' , final_std.shape)","metadata":{"scrolled":true,"execution":{"iopub.status.busy":"2021-07-22T07:40:17.823904Z","iopub.execute_input":"2021-07-22T07:40:17.824343Z","iopub.status.idle":"2021-07-22T07:40:22.669556Z","shell.execute_reply.started":"2021-07-22T07:40:17.824296Z","shell.execute_reply":"2021-07-22T07:40:22.668089Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"That is correct.\n\nFinally lets set our model.","metadata":{}},{"cell_type":"code","source":"## Creating our model object ##\n\nmodel = StyleTransfer()\n\nmodel = transfer_data(model , device)","metadata":{"execution":{"iopub.status.busy":"2021-07-22T07:40:22.671298Z","iopub.execute_input":"2021-07-22T07:40:22.672007Z","iopub.status.idle":"2021-07-22T07:40:24.646728Z","shell.execute_reply.started":"2021-07-22T07:40:22.671958Z","shell.execute_reply":"2021-07-22T07:40:24.645486Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Now lets set our loss function. We are going to use the Mean Squared Error loss function. So lets define that.","metadata":{}},{"cell_type":"code","source":"## Defining loss function ##\n\nmse_loss = nn.MSELoss()\n\n## Checking loss ##\n\ntest_lc = mse_loss(content_final_map , stylized_feature)\n\ntest_ls = mse_loss(style_mean , final_mean) + mse_loss(style_std , final_std)\n\ntotal_l = test_lc + 0.01 * test_ls\n\nprint('Content Loss : {}\\nStyle Loss : {}\\nTotal Loss : {}'.format(test_lc.item() , test_ls.item() , total_l.item()))","metadata":{"execution":{"iopub.status.busy":"2021-07-22T07:40:24.648524Z","iopub.execute_input":"2021-07-22T07:40:24.648976Z","iopub.status.idle":"2021-07-22T07:40:24.664482Z","shell.execute_reply.started":"2021-07-22T07:40:24.648933Z","shell.execute_reply":"2021-07-22T07:40:24.662792Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"So, everything is working fine.\n\nNow let's define our optimizer.","metadata":{}},{"cell_type":"code","source":"## Setting our optimizer ##\n\noptim = torch.optim.Adam(model.parameters() , lr = 1e-3)","metadata":{"execution":{"iopub.status.busy":"2021-07-22T07:40:24.666359Z","iopub.execute_input":"2021-07-22T07:40:24.667277Z","iopub.status.idle":"2021-07-22T07:40:24.677589Z","shell.execute_reply.started":"2021-07-22T07:40:24.667218Z","shell.execute_reply":"2021-07-22T07:40:24.676171Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Done.\n\nAll we do now is train and watch.","metadata":{}},{"cell_type":"code","source":"## Training our model ##\n\nnum_epochs = 10\n\nloop = tqdm(styletransfer_dl)\n\nfor epoch in range(num_epochs):\n    \n    for content , style in loop:\n        \n        stylized_img , content_final_map , stylized_feature , style_mean , style_std , final_mean , final_std = model(content ,\n                                                                                                                      style)\n        \n        lc = mse_loss(content_final_map , stylized_feature)\n\n        ls = mse_loss(style_mean , final_mean) + mse_loss(style_std , final_std)\n\n        total_loss = lc + 0.1 * ls\n        \n        optim.zero_grad()\n        \n        total_loss.backward()\n        \n        optim.step()\n        \n    print('Epoch : {} / {} --> Loss is {:.2f}'.format(epoch + 1 , num_epochs , total_loss.item()))\n    \n    visualize(content)\n    \n    visualize(style)\n    \n    visualize(stylized_img)","metadata":{"execution":{"iopub.status.busy":"2021-07-22T07:40:24.679417Z","iopub.execute_input":"2021-07-22T07:40:24.680153Z","iopub.status.idle":"2021-07-22T16:12:39.363802Z","shell.execute_reply.started":"2021-07-22T07:40:24.680105Z","shell.execute_reply":"2021-07-22T16:12:39.362738Z"},"trusted":true},"execution_count":null,"outputs":[]}]}