{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Basic Linear Regression with Pytorch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, in this notebook we are going to exercise our basic knowledge with implementation of own created Linear Regression using the basic syntax of Pytorch we learnt in the previous notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importing pytorch\n",
    "import torch\n",
    "\n",
    "import numpy as np #importing numpy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we can download a dataset easily from Kaggle and implement Linear Regression for sure, but in this basic level code lets create our own simple dataset and then carry on."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, we are going to have 5 samples of data with 3 features each. Furthermore, we are going to pop out 2 outputs.\n",
    "\n",
    "Lets do that first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.3708, 0.2472, 0.6404],\n",
       "        [0.5056, 0.4157, 1.0000],\n",
       "        [0.7191, 0.2809, 0.7640],\n",
       "        [0.4157, 0.3483, 0.4719],\n",
       "        [0.3258, 0.2247, 0.4607]])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "##Set the inputs\n",
    "\n",
    "inputs = torch.tensor([[33. , 22. , 57.],\n",
    "                       [45. , 37. , 89.],\n",
    "                       [64. , 25. , 68.],\n",
    "                       [37. , 31. , 42.],\n",
    "                       [29. , 20. , 41.]])\n",
    "\n",
    "inputs = inputs / torch.max(inputs)\n",
    "\n",
    "inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[45., 70.],\n",
       "        [27., 57.],\n",
       "        [34., 77.],\n",
       "        [41., 68.],\n",
       "        [37., 81.]])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "##Set the targets\n",
    "\n",
    "targets = torch.tensor([[45. , 70.],\n",
    "                        [27. , 57.],\n",
    "                        [34. , 77.],\n",
    "                        [41. , 68.], \n",
    "                        [37. , 81.]])\n",
    "\n",
    "targets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Voila! We have created our datasets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, a Linear Regression model is based on the equation of straight line, i.e., <code>y = mx + c</code>."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So we need to set our weights and biases.\n",
    "Lets do that.\n",
    "\n",
    "For this dataset, our weights would be of the shape (2,3) and our biases would be of the shape 2.\n",
    "\n",
    "Lets randomly define them to start our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "w = torch.randn(2 , 3 , requires_grad  = True)\n",
    "\n",
    "b = torch.randn(2 , requires_grad = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Done!\n",
    "\n",
    "The <code>requires_grad = True</code> sets the w and b for backprop.\n",
    "\n",
    "Now lets define our model and then define our loss function, thereafter start training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "##defining Linear Regression model\n",
    "\n",
    "def model(inputs):\n",
    "    return inputs @ w.t() + b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "##defining Cost Function\n",
    "\n",
    "def mse(t1 , t2):\n",
    "    \n",
    "    diff = t2 - t1\n",
    "    \n",
    "    return torch.sum(diff * diff) / diff.numel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-1.3214,  1.7636],\n",
      "        [-1.3020,  2.2316],\n",
      "        [-1.5479,  2.9163],\n",
      "        [-1.1105,  1.8982],\n",
      "        [-1.2462,  1.6030]], grad_fn=<AddBackward0>)\n"
     ]
    }
   ],
   "source": [
    "#Generate predictions\n",
    "preds = model(inputs)\n",
    "print(preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(3126.3918, grad_fn=<DivBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# Compute loss\n",
    "loss = mse(preds, targets)\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(3126.3918, grad_fn=<DivBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# Compute loss\n",
    "loss = mse(preds, targets)\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute gradients\n",
    "loss.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.6430,  1.6078, -0.4585],\n",
      "        [ 3.2840, -0.0061,  0.0731]], requires_grad=True)\n",
      "tensor([[-17.4033, -11.2928, -24.5240],\n",
      "        [-31.9236, -20.2625, -44.5688]])\n"
     ]
    }
   ],
   "source": [
    "# Gradients for weights\n",
    "print(w)\n",
    "print(w.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets now start training our model and see how it fetches!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch : [1 / 100] --> Loss : 3126.391845703125\n",
      "Epoch : [2 / 100] --> Loss : 3105.240234375\n",
      "Epoch : [3 / 100] --> Loss : 3094.730224609375\n",
      "Epoch : [4 / 100] --> Loss : 3084.25732421875\n",
      "Epoch : [5 / 100] --> Loss : 3073.82177734375\n",
      "Epoch : [6 / 100] --> Loss : 3063.423095703125\n",
      "Epoch : [7 / 100] --> Loss : 3053.061279296875\n",
      "Epoch : [8 / 100] --> Loss : 3042.736572265625\n",
      "Epoch : [9 / 100] --> Loss : 3032.4482421875\n",
      "Epoch : [10 / 100] --> Loss : 3022.1962890625\n",
      "Epoch : [11 / 100] --> Loss : 3011.98095703125\n",
      "Epoch : [12 / 100] --> Loss : 3001.8017578125\n",
      "Epoch : [13 / 100] --> Loss : 2991.65869140625\n",
      "Epoch : [14 / 100] --> Loss : 2981.5517578125\n",
      "Epoch : [15 / 100] --> Loss : 2971.48095703125\n",
      "Epoch : [16 / 100] --> Loss : 2961.445556640625\n",
      "Epoch : [17 / 100] --> Loss : 2951.44580078125\n",
      "Epoch : [18 / 100] --> Loss : 2941.481201171875\n",
      "Epoch : [19 / 100] --> Loss : 2931.552001953125\n",
      "Epoch : [20 / 100] --> Loss : 2921.65869140625\n",
      "Epoch : [21 / 100] --> Loss : 2911.80029296875\n",
      "Epoch : [22 / 100] --> Loss : 2901.9765625\n",
      "Epoch : [23 / 100] --> Loss : 2892.1875\n",
      "Epoch : [24 / 100] --> Loss : 2882.43359375\n",
      "Epoch : [25 / 100] --> Loss : 2872.71435546875\n",
      "Epoch : [26 / 100] --> Loss : 2863.02978515625\n",
      "Epoch : [27 / 100] --> Loss : 2853.37890625\n",
      "Epoch : [28 / 100] --> Loss : 2843.762451171875\n",
      "Epoch : [29 / 100] --> Loss : 2834.1806640625\n",
      "Epoch : [30 / 100] --> Loss : 2824.63232421875\n",
      "Epoch : [31 / 100] --> Loss : 2815.117919921875\n",
      "Epoch : [32 / 100] --> Loss : 2805.637451171875\n",
      "Epoch : [33 / 100] --> Loss : 2796.190673828125\n",
      "Epoch : [34 / 100] --> Loss : 2786.777099609375\n",
      "Epoch : [35 / 100] --> Loss : 2777.3974609375\n",
      "Epoch : [36 / 100] --> Loss : 2768.05126953125\n",
      "Epoch : [37 / 100] --> Loss : 2758.737548828125\n",
      "Epoch : [38 / 100] --> Loss : 2749.45703125\n",
      "Epoch : [39 / 100] --> Loss : 2740.20947265625\n",
      "Epoch : [40 / 100] --> Loss : 2730.99462890625\n",
      "Epoch : [41 / 100] --> Loss : 2721.812744140625\n",
      "Epoch : [42 / 100] --> Loss : 2712.66357421875\n",
      "Epoch : [43 / 100] --> Loss : 2703.546630859375\n",
      "Epoch : [44 / 100] --> Loss : 2694.462158203125\n",
      "Epoch : [45 / 100] --> Loss : 2685.40966796875\n",
      "Epoch : [46 / 100] --> Loss : 2676.389404296875\n",
      "Epoch : [47 / 100] --> Loss : 2667.401123046875\n",
      "Epoch : [48 / 100] --> Loss : 2658.44482421875\n",
      "Epoch : [49 / 100] --> Loss : 2649.520751953125\n",
      "Epoch : [50 / 100] --> Loss : 2640.62744140625\n",
      "Epoch : [51 / 100] --> Loss : 2631.766357421875\n",
      "Epoch : [52 / 100] --> Loss : 2622.936279296875\n",
      "Epoch : [53 / 100] --> Loss : 2614.137939453125\n",
      "Epoch : [54 / 100] --> Loss : 2605.37060546875\n",
      "Epoch : [55 / 100] --> Loss : 2596.634521484375\n",
      "Epoch : [56 / 100] --> Loss : 2587.929443359375\n",
      "Epoch : [57 / 100] --> Loss : 2579.255126953125\n",
      "Epoch : [58 / 100] --> Loss : 2570.611328125\n",
      "Epoch : [59 / 100] --> Loss : 2561.99853515625\n",
      "Epoch : [60 / 100] --> Loss : 2553.416259765625\n",
      "Epoch : [61 / 100] --> Loss : 2544.86474609375\n",
      "Epoch : [62 / 100] --> Loss : 2536.343017578125\n",
      "Epoch : [63 / 100] --> Loss : 2527.851806640625\n",
      "Epoch : [64 / 100] --> Loss : 2519.390625\n",
      "Epoch : [65 / 100] --> Loss : 2510.959716796875\n",
      "Epoch : [66 / 100] --> Loss : 2502.55859375\n",
      "Epoch : [67 / 100] --> Loss : 2494.187255859375\n",
      "Epoch : [68 / 100] --> Loss : 2485.845703125\n",
      "Epoch : [69 / 100] --> Loss : 2477.533447265625\n",
      "Epoch : [70 / 100] --> Loss : 2469.2509765625\n",
      "Epoch : [71 / 100] --> Loss : 2460.997802734375\n",
      "Epoch : [72 / 100] --> Loss : 2452.77392578125\n",
      "Epoch : [73 / 100] --> Loss : 2444.5791015625\n",
      "Epoch : [74 / 100] --> Loss : 2436.41357421875\n",
      "Epoch : [75 / 100] --> Loss : 2428.27685546875\n",
      "Epoch : [76 / 100] --> Loss : 2420.169189453125\n",
      "Epoch : [77 / 100] --> Loss : 2412.09033203125\n",
      "Epoch : [78 / 100] --> Loss : 2404.039794921875\n",
      "Epoch : [79 / 100] --> Loss : 2396.01806640625\n",
      "Epoch : [80 / 100] --> Loss : 2388.02490234375\n",
      "Epoch : [81 / 100] --> Loss : 2380.06005859375\n",
      "Epoch : [82 / 100] --> Loss : 2372.123046875\n",
      "Epoch : [83 / 100] --> Loss : 2364.214599609375\n",
      "Epoch : [84 / 100] --> Loss : 2356.333984375\n",
      "Epoch : [85 / 100] --> Loss : 2348.48193359375\n",
      "Epoch : [86 / 100] --> Loss : 2340.65673828125\n",
      "Epoch : [87 / 100] --> Loss : 2332.860107421875\n",
      "Epoch : [88 / 100] --> Loss : 2325.0908203125\n",
      "Epoch : [89 / 100] --> Loss : 2317.34912109375\n",
      "Epoch : [90 / 100] --> Loss : 2309.63525390625\n",
      "Epoch : [91 / 100] --> Loss : 2301.9482421875\n",
      "Epoch : [92 / 100] --> Loss : 2294.28857421875\n",
      "Epoch : [93 / 100] --> Loss : 2286.65625\n",
      "Epoch : [94 / 100] --> Loss : 2279.05126953125\n",
      "Epoch : [95 / 100] --> Loss : 2271.47265625\n",
      "Epoch : [96 / 100] --> Loss : 2263.92138671875\n",
      "Epoch : [97 / 100] --> Loss : 2256.39697265625\n",
      "Epoch : [98 / 100] --> Loss : 2248.898681640625\n",
      "Epoch : [99 / 100] --> Loss : 2241.427490234375\n",
      "Epoch : [100 / 100] --> Loss : 2233.98291015625\n"
     ]
    }
   ],
   "source": [
    "for epochs in range(100):\n",
    "    \n",
    "    \n",
    "    pred = model(inputs)\n",
    "    \n",
    "    pred.requires_grad_(True)\n",
    "    pred.retain_grad()\n",
    "    \n",
    "    loss = mse(pred , targets)\n",
    "    \n",
    "    #w.retain_grad()\n",
    "    #b.retain_grad()\n",
    "    \n",
    "    loss.backward() #backprop\n",
    "    \n",
    "    #print(loss.item())\n",
    "    \n",
    "    #print(w.grad)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "    \n",
    "        w -= (1e-3 * w.grad)\n",
    "        b -= (1e-3 * b.grad)\n",
    "    \n",
    "        w.grad.zero_()\n",
    "        b.grad.zero_()\n",
    "\n",
    "    \n",
    "    print('Epoch : [{} / {}] --> Loss : {}'.format(epochs + 1, 100 , loss.item()))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch_env",
   "language": "python",
   "name": "pytorch_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
