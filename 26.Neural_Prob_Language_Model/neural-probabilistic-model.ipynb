{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.10","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2023-05-12T12:02:30.152629Z","iopub.execute_input":"2023-05-12T12:02:30.152968Z","iopub.status.idle":"2023-05-12T12:02:30.166103Z","shell.execute_reply.started":"2023-05-12T12:02:30.152940Z","shell.execute_reply":"2023-05-12T12:02:30.164982Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"/kaggle/input/shakespeare-text/text.txt\n","output_type":"stream"}]},{"cell_type":"markdown","source":"#### Generation of Shakespeare like words through a simple Neural Network model.","metadata":{}},{"cell_type":"markdown","source":"This project is highly influenced by Andrej Karpathy's awesome [video](https://youtu.be/TCH_1BHY58I). This work would be very simple and will be mainly based on the re-implementation of this [paper](https://www.jmlr.org/papers/volume3/bengio03a/bengio03a.pdf).\n\nSo, without further adieu, lets get started with the necessary imports.","metadata":{}},{"cell_type":"code","source":"## Importing the necessary packages\n\nimport torch\nimport torch.nn as nn\nimport numpy as np\nfrom tqdm import tqdm","metadata":{"execution":{"iopub.status.busy":"2023-05-12T12:02:30.171334Z","iopub.execute_input":"2023-05-12T12:02:30.171842Z","iopub.status.idle":"2023-05-12T12:02:31.557691Z","shell.execute_reply.started":"2023-05-12T12:02:30.171804Z","shell.execute_reply":"2023-05-12T12:02:31.556680Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"markdown","source":"Since this would be very simplistic and mainly basic, we are just going to use the torch library to do everything.","metadata":{}},{"cell_type":"markdown","source":"##### Step 1: Dataset creation","metadata":{"execution":{"iopub.status.busy":"2023-05-12T09:00:34.299484Z","iopub.execute_input":"2023-05-12T09:00:34.299987Z","iopub.status.idle":"2023-05-12T09:00:34.305291Z","shell.execute_reply.started":"2023-05-12T09:00:34.299948Z","shell.execute_reply":"2023-05-12T09:00:34.303989Z"}}},{"cell_type":"markdown","source":"As a first step, we are going to build our dataset. Our dataset is going to be again very simple: \n- Given the text file, our dataset will read it, and make a vocabulary of unique words. Here I didn't remove the symbols so, they will also be part of the vocabulary.\n- Then from our vocabulary an index would be given to each word. We will do this via a dictionary mapping.\n- A reverse mapping will also be generated for the prediction stage.\n- Now, for the dataset, we will give 3 input words (X) and expect our model to generate a new word(Y).\n- We will also add a (< TOK >) word as a start sentence and end sentence character.  \n- Finally a train, validation and test split will also be generated to align with the proper deep learning project formulation.","metadata":{}},{"cell_type":"code","source":"def make_datasets(text_file, num_context = 3):\n    lines = []\n    # Reading the lines\n    with open(text_file, \"r\") as f:\n        for line in f.readlines():\n            line = \" \".join(line.lower().strip('\\n').split())\n            if (len(line) < 1) or (len(line) == 1 and line.isalnum() == False):\n                continue\n            lines.append(line)\n            \n    # Making the words vocabulary\n    words_vocabulary = set()\n    for each_line in lines:\n        words = each_line.split(' ')\n        for each_word in words:\n            if len(each_word) < 1:\n                continue\n            words_vocabulary.add(each_word)\n\n    words_vocabulary = sorted(list(words_vocabulary))\n\n    # Inserting the special token\n    words_vocabulary.insert(0 , \"<TOK>\")\n    \n    print(f'The vocabulary was created with {len(words_vocabulary)} words!!')\n    \n    # Making the word to index mapping\n    words_2_idx = {k:v for v,k in enumerate(words_vocabulary)}\n    \n    # Making index to word mapping\n    idx_2_words = {k:v for k,v in enumerate(words_vocabulary)}\n    \n    #Making entire dataset\n    X , y = [] , []\n    \n    for line in lines:\n        line_words = [\"<TOK>\"] * num_context + line.split(\" \") + [\"<TOK>\"]\n        for i in range(len(line_words) - num_context):\n            X.append([words_2_idx[w] for w in line_words[:num_context]])\n            if len(line_words[num_context]) < 1:\n                print(line_words)\n            y.append(words_2_idx[line_words[num_context]])\n            line_words = line_words[1:]\n    \n    X = torch.tensor(X)\n    y = torch.tensor(y)\n    \n    print(X.shape)\n    \n    # Making datasets\n    indices = np.arange(0,len(X))\n    np.random.shuffle(indices)\n    \n    n1 = int(0.8 * len(X))\n    n2 = int(0.9 * len(X))\n    \n    train_indices = indices[:n1]\n    val_indices = indices[n1:n2]\n    test_indices = indices[n2:]\n    \n    train_dataset = X[train_indices], y[train_indices]\n    val_dataset = X[val_indices], y[val_indices]\n    test_dataset = X[test_indices], y[test_indices]\n    \n    print(f'Train dataset has {len(train_dataset[0])} datapoints.')\n    print(f'Val dataset has {len(val_dataset[0])} datapoints.')\n    print(f'Test dataset has {len(test_dataset[0])} datapoints.')\n    \n    return train_dataset, val_dataset, test_dataset , words_vocabulary, words_2_idx , idx_2_words","metadata":{"execution":{"iopub.status.busy":"2023-05-12T12:02:31.558799Z","iopub.execute_input":"2023-05-12T12:02:31.559260Z","iopub.status.idle":"2023-05-12T12:02:31.574102Z","shell.execute_reply.started":"2023-05-12T12:02:31.559236Z","shell.execute_reply":"2023-05-12T12:02:31.572933Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"train_dataset, val_dataset, test_dataset, vocabulary, words_2_idx ,  idx_2_words= make_datasets(\n    '/kaggle/input/shakespeare-text/text.txt'\n)","metadata":{"execution":{"iopub.status.busy":"2023-05-12T12:02:31.577633Z","iopub.execute_input":"2023-05-12T12:02:31.578566Z","iopub.status.idle":"2023-05-12T12:02:32.668081Z","shell.execute_reply.started":"2023-05-12T12:02:31.578496Z","shell.execute_reply":"2023-05-12T12:02:32.666967Z"},"trusted":true},"execution_count":4,"outputs":[{"name":"stdout","text":"The vocabulary was created with 23642 words!!\ntorch.Size([235428, 3])\nTrain dataset has 188342 datapoints.\nVal dataset has 23543 datapoints.\nTest dataset has 23543 datapoints.\n","output_type":"stream"}]},{"cell_type":"code","source":"# Creation of dataloader instance\n\ntrain_dl = torch.utils.data.DataLoader(train_dataset, batch_size=128, shuffle=True)\nval_dl = torch.utils.data.DataLoader(val_dataset, batch_size=128, shuffle=True)\ntest_dl = torch.utils.data.DataLoader(test_dataset, batch_size=128, shuffle=True)","metadata":{"execution":{"iopub.status.busy":"2023-05-12T12:02:32.669175Z","iopub.execute_input":"2023-05-12T12:02:32.669527Z","iopub.status.idle":"2023-05-12T12:02:32.675612Z","shell.execute_reply.started":"2023-05-12T12:02:32.669504Z","shell.execute_reply":"2023-05-12T12:02:32.674490Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"markdown","source":"Trust me it took me several attempts to actually make it. So dont be harsh on yourself if you cannot do it in the first go. Just be patient and do each part one at a time. I also did like that, and removed the testing stuff after my work was done.","metadata":{}},{"cell_type":"markdown","source":"##### Step 2: Simple model creation","metadata":{}},{"cell_type":"markdown","source":"In this step we are going to create our model. Its going to be very simple as usual. \n\nSo what are the steps?\n- First up we will make an embedding layer. This embedding layer will be basically a lookup table. What this embedding layer will hold is very simple. It will actually be a layer of shape (num_words_in_vocab, feature_dim), where we compress our words to be represented in a smaller feature space. You might wonder what's the difference and this is going to help? The fact is when we do processing of words via a neural network, we put in numbers instead of words. As of now we changed the words to a number like 1,2,... But the thing is we cant pass 1,2,3... values as these represent ordinal values, i.e., higher values have more precedence. But, in case of words this should not be the case and each word must be represented as an independed nominal vector. So, the easiest way to do this is to convert the 1,2,3... into one hot vectors, with each other having a size of (num_words_in_vocab) with all zeros except at one position (its index). So, for the entire dataset, that would be (num_words_in_vocab,num_words_in_vocab) which would be bizarrely high. So to compress this we compress every word in a lower dimensional space with a smaller feature vector. This is done by the embedding layer, which is learnable.\n\n- Next up we need to map this to a hidden layer with n neurons and pass it through a tanh function.\n\n- Finally we would output to another layer, the output layer which must have the size of the vocabulary to represent each of the next word.\n\n- Using the final output layer we would plug in our loss function and do our optimization.","metadata":{}},{"cell_type":"code","source":"## The model\n\nclass NP_Model(nn.Module):\n    \"\"\"Pytorch model\"\"\"\n    \n    def __init__(self , vocab_size, embedding_dim, hidden_dim, num_context=3):\n        super().__init__()\n        self.model = nn.Sequential(\n            nn.Embedding(vocab_size , embedding_dim), #give (B, num_context, embedding_dim)\n            nn.Flatten(), #makes (B, num_context, embedding_dim)\n            nn.Linear(num_context * embedding_dim , hidden_dim),\n            nn.Tanh(),\n            nn.Linear(hidden_dim , vocab_size)\n        )\n        \n    def forward(self , x):\n        return self.model(x)","metadata":{"execution":{"iopub.status.busy":"2023-05-12T12:02:32.677264Z","iopub.execute_input":"2023-05-12T12:02:32.677595Z","iopub.status.idle":"2023-05-12T12:02:32.688963Z","shell.execute_reply.started":"2023-05-12T12:02:32.677570Z","shell.execute_reply":"2023-05-12T12:02:32.687935Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"markdown","source":"Let's test our model too.","metadata":{}},{"cell_type":"code","source":"model = NP_Model(vocab_size = len(vocabulary), embedding_dim = 30 , hidden_dim = 100)\n\nmodel(train_dataset[0][3].unsqueeze(0)).shape","metadata":{"execution":{"iopub.status.busy":"2023-05-12T12:02:32.690738Z","iopub.execute_input":"2023-05-12T12:02:32.691142Z","iopub.status.idle":"2023-05-12T12:02:32.734962Z","shell.execute_reply.started":"2023-05-12T12:02:32.691111Z","shell.execute_reply":"2023-05-12T12:02:32.733747Z"},"trusted":true},"execution_count":7,"outputs":[{"execution_count":7,"output_type":"execute_result","data":{"text/plain":"torch.Size([1, 23642])"},"metadata":{}}]},{"cell_type":"markdown","source":"Perfect....\nOur model works as expected.\n\nNow lets device our loss function and our optimizer.","metadata":{}},{"cell_type":"code","source":"## Re-initializing our mode\n\nmodel = NP_Model(vocab_size = len(vocabulary), embedding_dim = 30 , hidden_dim = 100)\n\n## Loss function\n\nloss_func = nn.CrossEntropyLoss()\n\noptim = torch.optim.Adam(model.parameters() , lr=0.1)","metadata":{"execution":{"iopub.status.busy":"2023-05-12T12:02:32.736155Z","iopub.execute_input":"2023-05-12T12:02:32.736478Z","iopub.status.idle":"2023-05-12T12:02:32.768846Z","shell.execute_reply.started":"2023-05-12T12:02:32.736445Z","shell.execute_reply":"2023-05-12T12:02:32.767642Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"markdown","source":"Now we are ready. Let's get to training.","metadata":{}},{"cell_type":"code","source":"## Training on minibatches\n\nlosses = []\nX_tr,y_tr = train_dataset\n\nloop = tqdm(range(10000))\nfor i in loop:\n    idx = np.random.randint(0, len(X_tr), 32)\n    X , y = X_tr[idx] , y_tr[idx]\n    pred = model(X)\n    loss = loss_func(pred,y)\n    losses.append(loss.item())\n    loop.set_description(f'Epoch : {i+1}/100 :')\n    loop.set_postfix(Loss=loss.item())\n    losses.append(loss.item())\n    optim.zero_grad()\n    loss.backward()\n    optim.step()","metadata":{"execution":{"iopub.status.busy":"2023-05-12T12:02:32.769992Z","iopub.execute_input":"2023-05-12T12:02:32.770273Z","iopub.status.idle":"2023-05-12T12:06:10.165356Z","shell.execute_reply.started":"2023-05-12T12:02:32.770251Z","shell.execute_reply":"2023-05-12T12:06:10.164143Z"},"trusted":true},"execution_count":9,"outputs":[{"name":"stderr","text":"Epoch : 10000/100 :: 100%|██████████| 10000/10000 [03:37<00:00, 46.00it/s, Loss=58.3]\n","output_type":"stream"}]},{"cell_type":"markdown","source":"Model evaluation.","metadata":{}},{"cell_type":"code","source":"## Model evaluation\n\nX_val, y_val = val_dataset\nX_val = X_val[:1000]\ny_val = y_val[:1000]\nmodel.eval()\npred = model(X_val)\nloss = loss_func(pred,y_val)\nprint(f'Evaluation Loss : {loss.item()}')","metadata":{"execution":{"iopub.status.busy":"2023-05-12T12:06:10.166842Z","iopub.execute_input":"2023-05-12T12:06:10.167193Z","iopub.status.idle":"2023-05-12T12:06:10.276850Z","shell.execute_reply.started":"2023-05-12T12:06:10.167166Z","shell.execute_reply":"2023-05-12T12:06:10.275717Z"},"trusted":true},"execution_count":10,"outputs":[{"name":"stdout","text":"Evaluation Loss : 76.62408447265625\n","output_type":"stream"}]},{"cell_type":"markdown","source":"I implemented a very basic level model.\n\nMainly because of the memory consumption.\n\nThis was done as just as an introductory work to learn about transformers.","metadata":{}}]}